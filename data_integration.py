# -*- coding: utf-8 -*-
"""Data_Integration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q6eFMmj2e5ig8LE7uOH4GKXef9ND40WB

# **Project**

**DATA INTEGRATION**
"""

#!pip install mlxtend==0.19.0

import numpy as np
import pandas as pd
import numpy as np
import os
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten
from sklearn.neighbors import KNeighborsClassifier
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, LSTM, Input, Lambda, Dense
import tensorflow.keras.backend as K

import pandas as pd

# Load the datasets
user_df = pd.read_excel("https://raw.githubusercontent.com/Vijay-602/Data-mining/main/Tourism_Dataset/User.xlsx")
transaction_df = pd.read_excel("https://raw.githubusercontent.com/Vijay-602/Data-mining/main/Tourism_Dataset/Transaction.xlsx")

# Merge the datasets on 'UserId' column to include location details in the transaction data
merged_df = pd.merge(transaction_df, user_df, on="UserId", how="left")

# Display the first few rows of the merged dataset to verify
print(merged_df.head())

# Load Item.xlsx for attraction details
item_df = pd.read_excel("https://raw.githubusercontent.com/Vijay-602/Data-mining/main/Tourism_Dataset/Item.xlsx")

# Merge the dataset from Step 1 with Item.xlsx based on 'AttractionId'
final_merged_df = pd.merge(merged_df, item_df, on="AttractionId", how="left")

print("Columns in final_merged_df before merge:", final_merged_df.columns)

# Display the first few rows of the final merged dataset
print(final_merged_df.head())

# Load Mode.xlsx and Type.xlsx for mapping VisitMode and AttractionType labels
mode_df = pd.read_excel("https://raw.githubusercontent.com/Vijay-602/Data-mining/main/Tourism_Dataset/Mode.xlsx")
type_df = pd.read_excel("https://raw.githubusercontent.com/Vijay-602/Data-mining/main/Tourism_Dataset/Type.xlsx")

#final_merged_df = pd.merge(final_merged_df, continent_df, on="ContenentId", how="left")

# Merge final_merged_df with mode_df to map VisitModeId to VisitMode (descriptive labels)
final_merged_df = pd.merge(final_merged_df, mode_df, left_on="VisitModeId", right_on="VisitModeId", how="left")

# Merge final_merged_df with type_df to map AttractionTypeId to AttractionType (descriptive labels)
final_merged_df = pd.merge(final_merged_df, type_df, left_on="AttractionTypeId", right_on="AttractionTypeId", how="left")

# Drop redundant ID columns if desired (optional)
final_merged_df.drop(columns=["VisitModeId", "AttractionTypeId"], inplace=True)

print("Columns in final_merged_df before merge:", final_merged_df.columns)

# Display the first few rows of the updated final_merged_df
print(final_merged_df.head())

final_merged_df.to_csv("Integrated_data.csv")

import pandas as pd

# Load each dataset from the GitHub repository
continent_df = pd.read_excel("https://raw.githubusercontent.com/Vijay-602/Data-mining/main/Tourism_Dataset/Continent.xlsx")
country_df = pd.read_excel("https://raw.githubusercontent.com/Vijay-602/Data-mining/main/Tourism_Dataset/Country.xlsx")
region_df = pd.read_excel("https://raw.githubusercontent.com/Vijay-602/Data-mining/main/Tourism_Dataset/Region.xlsx")
city_df = pd.read_excel("https://raw.githubusercontent.com/Vijay-602/Data-mining/main/Tourism_Dataset/City.xlsx")


print("Columns in region_df:", region_df.columns)
print("Columns in continent_df:", continent_df.columns)

# Step 1: Merge Region with Continent on 'ContenentId' to add continent information to regions
region_continent_df = pd.merge(region_df, continent_df, on="ContenentId", how="left")

# Step 2: Merge Country with Region-Continent Data on 'RegionId' to add region and continent information to countries
country_region_continent_df = pd.merge(country_df, region_continent_df, on="RegionId", how="left")

# Step 3: Merge City with Country-Region-Continent Data on 'CountryId' to add country, region, and continent information to cities
location_hierarchy_df = pd.merge(city_df, country_region_continent_df, on="CountryId", how="left")

# Display the first few rows of the final merged location hierarchy
print(location_hierarchy_df.head())

import pandas as pd
import matplotlib.pyplot as plt

# Assuming final_merged_df is the merged dataframe with all relevant details

# Step 1: Calculate the average rating for each attraction
attraction_ratings = final_merged_df.groupby('Attraction')['Rating'].mean().reset_index()

# Step 2: Sort attractions by average rating in descending order and select the top 10
top_attractions = attraction_ratings.sort_values(by='Rating', ascending=False).head(30)

# Step 3: Plot the top attractions based on average rating
plt.figure(figsize=(30, 6))
plt.barh(top_attractions['Attraction'], top_attractions['Rating'], color='skyblue')
plt.xlabel('Average Rating')
plt.title('Top 30 Visited Attractions by Average Rating')
plt.gca().invert_yaxis()  # To display the highest-rated attractions at the top
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming final_merged_df contains columns for Region, Country, and relevant transaction information

# Step 1: Count the visits by region and country
region_country_visits = final_merged_df.groupby(['RegionId', 'CountryId']).size().reset_index(name='VisitCount')

# Step 2: Pivot the data to create a matrix for the heatmap
region_country_pivot = region_country_visits.pivot(index="RegionId", columns="CountryId", values="VisitCount")

# Step 3: Plot the heatmap with improvements
plt.figure(figsize=(20, 10))  # Increase the figure size
sns.heatmap(region_country_pivot, cmap="YlGnBu", annot=True, fmt=".0f", cbar_kws={'label': 'Visit Count'},
            linewidths=0.5, linecolor='gray', annot_kws={"size": 8})
plt.xlabel('Country ID', fontsize=12)
plt.ylabel('Region ID', fontsize=12)
plt.title('Visits by Region and Country', fontsize=15)

# Rotate the x and y tick labels for better readability
plt.xticks(rotation=45, ha='right', fontsize=8)
plt.yticks(rotation=0, fontsize=8)

plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Assuming final_merged_df contains columns for VisitMonth and TransactionId

# Step 1: Count the number of visits per month
monthly_visits = final_merged_df.groupby('VisitMonth').size().reset_index(name='VisitCount')

# Step 2: Sort the months in chronological order (if they are not already)
monthly_visits = monthly_visits.sort_values(by='VisitMonth')

# Step 3: Plot the line graph for monthly visit trends
plt.figure(figsize=(10, 6))
plt.plot(monthly_visits['VisitMonth'], monthly_visits['VisitCount'], marker='o', linestyle='-', color='b')
plt.xlabel('Month')
plt.ylabel('Visit Count')
plt.title('Seasonal Trends in Visits')
plt.xticks(ticks=range(1, 13), labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])
plt.grid(True)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Assuming final_merged_df contains columns for VisitMode and Attraction

# Step 1: Count the occurrences of each VisitMode
visit_mode_counts = final_merged_df['VisitMode'].value_counts().reset_index()
visit_mode_counts.columns = ['VisitMode', 'Count']

# Step 2: Plot a pie chart for Visit Mode distribution
plt.figure(figsize=(8, 8))
plt.pie(visit_mode_counts['Count'], labels=visit_mode_counts['VisitMode'], autopct='%1.1f%%', startangle=140, colors=['#66c2a5','#fc8d62','#8da0cb','#e78ac3','#a6d854'])
plt.title('Distribution of Visit Modes')
plt.show()

# Alternatively, you can plot a bar chart

# Step 3: Plot a bar chart for Visit Mode distribution
plt.figure(figsize=(10, 6))
plt.bar(visit_mode_counts['VisitMode'], visit_mode_counts['Count'], color='skyblue')
plt.xlabel('Visit Mode')
plt.ylabel('Count')
plt.title('Visit Mode Distribution')
plt.show()

import pandas as pd

# Load final_merged_df as well as the lookup tables
city_df = pd.read_excel("https://raw.githubusercontent.com/Vijay-602/Data-mining/main/Tourism_Dataset/City.xlsx")
continent_df = pd.read_excel("https://raw.githubusercontent.com/Vijay-602/Data-mining/main/Tourism_Dataset/Continent.xlsx")
country_df = pd.read_excel("https://raw.githubusercontent.com/Vijay-602/Data-mining/main/Tourism_Dataset/Country.xlsx")
region_df = pd.read_excel("https://raw.githubusercontent.com/Vijay-602/Data-mining/main/Tourism_Dataset/Region.xlsx")

# Create mapping dictionaries for each ID column to its name
continent_map = dict(zip(continent_df['ContenentId'], continent_df['Contenent']))
country_map = dict(zip(country_df['CountryId'], country_df['Country']))
region_map = dict(zip(region_df['RegionId'], region_df['Region']))
city_map = dict(zip(city_df['CityId'], city_df['CityName']))

# Replace the ID columns in final_merged_df with their names
final_merged_df['ContenentId'] = final_merged_df['ContenentId'].map(continent_map)
final_merged_df['CountryId'] = final_merged_df['CountryId'].map(country_map)
final_merged_df['RegionId'] = final_merged_df['RegionId'].map(region_map)
final_merged_df['CityId'] = final_merged_df['CityId'].map(city_map)

# Rename columns for clarity
final_merged_df.rename(columns={
    'ContenentId': 'Continent',
    'CountryId': 'Country',
    'RegionId': 'Region',
    'CityId': 'City'
}, inplace=True)

# Display the first few rows of the updated final_merged_df
print(final_merged_df.head())

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming final_merged_df contains columns for Continent and VisitMode

# Step 1: Create a cross-tabulation between Continent and VisitMode


demographics_behavior = pd.crosstab(final_merged_df['Continent'], final_merged_df['VisitMode'])

# Step 2: Plot a stacked bar chart for user demographics and travel behavior
demographics_behavior.plot(kind='bar', stacked=True, figsize=(12, 8), color=['#66c2a5','#fc8d62','#8da0cb','#e78ac3','#a6d854'])
plt.xlabel('Continent')
plt.ylabel('Visit Mode Count')
plt.title('User Demographics and Travel Behavior (Stacked by Visit Mode)')
plt.legend(title='Visit Mode')
plt.show()

# Alternatively, you can plot a heatmap to visualize the correlation

# Step 3: Plot a heatmap for user demographics and travel behavior
plt.figure(figsize=(12, 8))
sns.heatmap(demographics_behavior, annot=True, fmt="d", cmap="YlGnBu", cbar_kws={'label': 'Visit Mode Count'})
plt.xlabel('Visit Mode')
plt.ylabel('Continent')
plt.title('User Demographics and Travel Behavior Heatmap')
plt.show()

final_merged_df.drop(columns=["AttractionId", "AttractionCityId"], inplace=True)

print(final_merged_df.head())

# Save the final_merged_df as a CSV file
final_merged_df.to_csv("final_merged_tourism_data.csv", index=False)

import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load the final merged data (assuming final_merged_df is ready for clustering)
# Keep the original categorical columns for later interpretation and only encode copies for clustering

# Encode categorical variables for clustering purposes only
final_merged_df['Continent_encoded'] = LabelEncoder().fit_transform(final_merged_df['Continent'])
final_merged_df['Country_encoded'] = LabelEncoder().fit_transform(final_merged_df['Country'])
final_merged_df['VisitMode_encoded'] = LabelEncoder().fit_transform(final_merged_df['VisitMode'])
final_merged_df['AttractionType_encoded'] = LabelEncoder().fit_transform(final_merged_df['AttractionType'])

# Encode VisitMonth as cyclic features
final_merged_df['VisitMonth_sin'] = np.sin(2 * np.pi * final_merged_df['VisitMonth'] / 12)
final_merged_df['VisitMonth_cos'] = np.cos(2 * np.pi * final_merged_df['VisitMonth'] / 12)

# Select relevant columns for clustering, using encoded versions of categorical features
clustering_data = final_merged_df[['Continent_encoded', 'Country_encoded', 'VisitMode_encoded', 'AttractionType_encoded', 'VisitMonth_sin', 'VisitMonth_cos']]

# Standardize the data
scaler = StandardScaler()
clustering_data_scaled = scaler.fit_transform(clustering_data)

# Determine the optimal number of clusters using the elbow method
wcss = []  # Within-cluster sum of squares
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(clustering_data_scaled)
    wcss.append(kmeans.inertia_)

# Plot the elbow curve
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), wcss, marker='o', linestyle='-')
plt.title('Elbow Method For Optimal k')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Apply K-means with the chosen number of clusters (e.g., k=4 based on the elbow method)
kmeans = KMeans(n_clusters=4, random_state=42)
final_merged_df['Cluster'] = kmeans.fit_predict(clustering_data_scaled)

# Analyze cluster assignments
# Show the number of users in each cluster
print(final_merged_df['Cluster'].value_counts())

# Use the encoded columns instead of the original categorical columns for pairplot
sns.pairplot(final_merged_df, hue='Cluster', vars=['Continent_encoded', 'Country_encoded', 'VisitMode_encoded', 'AttractionType_encoded', 'VisitMonth_sin', 'VisitMonth_cos'])
plt.show()

#!pip install mlxtend==0.19.0

'''
import os
os.kill(os.getpid(), 9)
'''

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from mlxtend.frequent_patterns import apriori, association_rules

# Prepare data for association rules analysis
encoded_df = final_merged_df[['Country', 'VisitMode', 'AttractionType']].copy()

# Convert columns to categorical types
encoded_df = encoded_df.astype('category')

# Convert data into one-hot encoding
onehot_df = pd.get_dummies(encoded_df)

# Apply Apriori to generate frequent itemsets
frequent_itemsets = apriori(onehot_df, min_support=0.01, use_colnames=True)

# Debug: Check the structure of frequent_itemsets
print("\nFrequent Itemsets DataFrame Columns:", frequent_itemsets.columns)
print(frequent_itemsets.head())

# Ensure 'support' and 'itemsets' columns exist
if 'support' in frequent_itemsets.columns and 'itemsets' in frequent_itemsets.columns:
    if not frequent_itemsets.empty:
        try:
            # Generate association rules
            rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)
            print("\nGenerated Association Rules:")
            print(rules.head())

            # If rules are generated, sort and plot the top ones
            if not rules.empty:
                rules = rules.sort_values(by=['confidence', 'lift'], ascending=False).head(10)

                # Plot top rules by confidence
                plt.figure(figsize=(10, 6))
                sns.barplot(x='confidence', y=rules['antecedents'].apply(lambda x: ', '.join(list(x))), orient='h')
                plt.title("Top 10 Association Rules by Confidence")
                plt.xlabel("Confidence")
                plt.ylabel("Antecedents")
                plt.show()
            else:
                print("No association rules were generated.")
        except TypeError as e:
            print(f"Error generating association rules: {e}")
        except Exception as e:
            print(f"Unexpected error: {e}")
    else:
        print("No frequent itemsets were generated.")
else:
    print("Frequent itemsets DataFrame is missing required columns ('support', 'itemsets').")

#!pip install mlxtend==0.19.0

import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules

# One-hot encode AttractionType, VisitMode, and VisitMonth
# This ensures each month is converted to its own binary column (e.g., Month_1, Month_2, ..., Month_12)
one_hot_data = pd.get_dummies(final_merged_df[['AttractionType', 'VisitMode', 'VisitMonth']],
                              columns=['AttractionType', 'VisitMode', 'VisitMonth'])

# Apply Apriori with lower minimum support
frequent_itemsets = apriori(one_hot_data, min_support=0.005, use_colnames=True)

# Generate association rules with lower confidence threshold
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.4)


# Filter rules to include patterns that combine AttractionType, VisitMode, and VisitMonth
filtered_rules = rules[
    (rules['antecedents'].apply(lambda x: any("AttractionType" in item for item in x))) &
    (rules['antecedents'].apply(lambda x: any("VisitMode" in item for item in x))) |
    (rules['consequents'].apply(lambda x: any("VisitMonth" in item for item in x)))
]


# Display the filtered association rules
print("All Association Rules:")
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(20))  # Display the top 20 rules for a quick overview

print(final_merged_df.head(5))

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'rules' contains the full set of association rules from the previous code
# Extract top 10 rules by confidence for a quick summary
top_rules = rules.nlargest(10, 'confidence')[['antecedents', 'consequents', 'support', 'confidence', 'lift']]

# Display the top rules as a summary table
print("Summary of Top 10 Association Rules by Confidence:")
print(top_rules)

# Visualization 1: Bar Chart of Top Rules by Confidence
plt.figure(figsize=(12, 6))
sns.barplot(x=top_rules['confidence'], y=top_rules['antecedents'].astype(str) + ' → ' + top_rules['consequents'].astype(str))
plt.title('Top 10 Association Rules by Confidence')
plt.xlabel('Confidence')
plt.ylabel('Rule')
plt.show()

# Visualization 2: Scatter Plot of Support vs Confidence, with Lift as Color
plt.figure(figsize=(10, 6))
scatter = plt.scatter(rules['support'], rules['confidence'], c=rules['lift'], cmap='viridis', alpha=0.7)
plt.colorbar(scatter, label='Lift')
plt.title('Support vs Confidence of Association Rules')
plt.xlabel('Support')
plt.ylabel('Confidence')
plt.show()

!pip install scikit-surprise

import pandas as pd
from surprise import SVD, Dataset, Reader
from surprise.model_selection import train_test_split
from surprise import accuracy
from surprise.model_selection import cross_validate

# Assuming `final_merged_df` contains columns: UserId, Attraction, and Rating

# Prepare the data for Surprise library
# Use only relevant columns for collaborative filtering
df = final_merged_df[['UserId', 'Attraction', 'Rating']]

# Define a reader for the dataset with the rating scale (assuming ratings are from 1 to 5)
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(df, reader)

# Split the data into training and test sets
trainset, testset = train_test_split(data, test_size=0.25, random_state=42)

# Initialize the SVD model
svd = SVD()

# Train the model
svd.fit(trainset)

# Test the model
predictions = svd.test(testset)
accuracy.rmse(predictions)

# Cross-validate to see average performance (optional)
cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

# Function to recommend top attractions for a given user based on predicted ratings
def get_recommendations(user_id, num_recommendations=5):
    # Get all unique attractions
    all_attractions = df['Attraction'].unique()

    # Get attractions the user has already rated
    user_rated_attractions = df[df['UserId'] == user_id]['Attraction'].unique()

    # Filter out attractions the user has already rated
    attractions_to_predict = [attr for attr in all_attractions if attr not in user_rated_attractions]

    # Predict ratings for all unrated attractions
    predictions = [svd.predict(user_id, attraction) for attraction in attractions_to_predict]

    # Sort predictions by estimated rating
    recommendations = sorted(predictions, key=lambda x: x.est, reverse=True)

    # Get top-N recommendations
    top_recommendations = [(pred.iid, round(pred.est, 2)) for pred in recommendations[:num_recommendations]]

    return top_recommendations

# Example: Get top 5 recommendations for a user
user_id = 70456  # Replace with the desired user ID
top_recommendations = get_recommendations(user_id, num_recommendations=5)
print(f"Top 5 recommendations for user {user_id}:")
for attraction, rating in top_recommendations:
    print(f"Attraction: {attraction}, Predicted Rating: {rating}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from surprise import SVD, Dataset, Reader
from surprise.model_selection import train_test_split, cross_validate

# Assuming cross-validation and recommendations have already been done
# and 'top_recommendations' contains recommendations for a specific user.

# Cross-validation results from Surprise (assuming we stored it as 'cv_results')
# Sample result from cross_validate
cv_results = {
    "Fold": ["Fold 1", "Fold 2", "Fold 3", "Fold 4", "Fold 5"],
    "RMSE": [0.9145, 0.9114, 0.9133, 0.9095, 0.9101],
    "MAE": [0.7161, 0.7143, 0.7153, 0.7145, 0.7110]
}
cv_df = pd.DataFrame(cv_results)

# Plot RMSE and MAE across each fold
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.barplot(x="Fold", y="RMSE", data=cv_df)
plt.title("RMSE Across Cross-Validation Folds")
plt.xlabel("Fold")
plt.ylabel("RMSE")

plt.subplot(1, 2, 2)
sns.barplot(x="Fold", y="MAE", data=cv_df)
plt.title("MAE Across Cross-Validation Folds")
plt.xlabel("Fold")
plt.ylabel("MAE")
plt.tight_layout()
plt.show()

# Visualization 2: Distribution of Prediction Errors (Residuals)
# Calculate residuals
residuals = [pred.r_ui - pred.est for pred in predictions]
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True, bins=20)
plt.title("Distribution of Prediction Errors (Residuals)")
plt.xlabel("Error (Actual - Predicted)")
plt.ylabel("Frequency")
plt.show()

# Visualization 3: Bar Chart for Top Recommendations for a User
# Convert 'top_recommendations' to a DataFrame for easier plotting
recommendations_df = pd.DataFrame(top_recommendations, columns=['Attraction', 'Predicted Rating'])

plt.figure(figsize=(12, 6))
sns.barplot(x="Predicted Rating", y="Attraction", data=recommendations_df)
plt.title(f"Top {len(top_recommendations)} Recommended Attractions for User {user_id}")
plt.xlabel("Predicted Rating")
plt.ylabel("Attraction")
plt.xlim(4, 5)  # assuming ratings are between 4 and 5
plt.show()

print(final_merged_df.head())

import pandas as pd

# Assuming 'final_merged_df' contains the merged dataset
# Step 1: Create a User Profile based on preferred AttractionType

# Count the number of visits per AttractionType for each user
user_preference = final_merged_df.groupby(['UserId', 'AttractionType']).size().unstack(fill_value=0)

# Identify the most visited AttractionType(s) for each user
user_preference['PreferredAttractionType'] = user_preference.idxmax(axis=1)

# Merge the preferred attraction type back with the original dataframe
final_merged_df = final_merged_df.merge(user_preference['PreferredAttractionType'], on='UserId')

# Step 2: Content-Based Filtering Function to Recommend Similar Attractions
def recommend_attractions(user_id, num_recommendations=5):
    # Get user's preferred attraction type
    preferred_type = user_preference.loc[user_id, 'PreferredAttractionType']

    # Filter attractions by the preferred type
    recommendations = final_merged_df[
        (final_merged_df['AttractionType'] == preferred_type) &
        (final_merged_df['UserId'] != user_id)  # Exclude attractions the user has already visited
    ]

    # Get unique recommendations and limit to the specified number
    recommended_attractions = recommendations[['Attraction', 'AttractionType']].drop_duplicates().head(num_recommendations)

    return recommended_attractions

# Example usage: Get top 5 recommended attractions for a specific user
user_id = 70456  # Replace with the desired user ID
top_recommendations = recommend_attractions(user_id, num_recommendations=5)

print(f"Top recommendations for user {user_id} based on their interest in '{user_preference.loc[user_id, 'PreferredAttractionType']}':")
print(top_recommendations)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Visualize User Preferences by Attraction Type
# Count the number of visits to each AttractionType for each user and plot for the selected user
user_id = 70456  # Replace with desired user ID

# Filter data for the selected user
user_data = final_merged_df[final_merged_df['UserId'] == user_id]

# Count the number of visits per AttractionType for the selected user
user_attraction_counts = user_data['AttractionType'].value_counts()

# Plot user preference as a bar chart
plt.figure(figsize=(10, 5))
sns.barplot(x=user_attraction_counts.index, y=user_attraction_counts.values)
plt.title(f"User {user_id}'s Attraction Type Preferences")
plt.xlabel("Attraction Type")
plt.ylabel("Number of Visits")
plt.xticks(rotation=45)
plt.show()

# Step 2: Visualize Recommended Attractions for the User
# Convert recommendations to a DataFrame for easier plotting
top_recommendations = recommend_attractions(user_id, num_recommendations=5)
recommendations_df = pd.DataFrame(top_recommendations)

# Plot the recommended attractions in a bar chart
plt.figure(figsize=(10, 5))
sns.barplot(x="Attraction", y="AttractionType", data=recommendations_df, palette="viridis")
plt.title(f"Top {len(recommendations_df)} Recommendations for User {user_id}")
plt.xlabel("Recommended Attraction")
plt.ylabel("Attraction Type")
plt.xticks(rotation=45)
plt.show()

print(final_merged_df.head())

final_df = pd.read_csv("/content/final_merged_tourism_data.csv")

final_df.head()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming `final_merged_df` is the final dataframe

# Step 1: Bar Chart of Top Attractions by Continent
# Count visits to each attraction by continent
attraction_continent_counts = final_merged_df.groupby(['Continent', 'Attraction']).size().reset_index(name='VisitCount')

# Plot the bar chart
plt.figure(figsize=(14, 8))
sns.barplot(data=attraction_continent_counts, x='VisitCount', y='Attraction', hue='Continent', dodge=False)
plt.title('Top Attractions Visited by Users from Different Continents')
plt.xlabel('Number of Visits')
plt.ylabel('Attraction')
plt.legend(title='Continent', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

# Step 2: Heatmap of Top Attractions by Country
# Count visits to each attraction by country
attraction_country_counts = final_merged_df.groupby(['Country', 'Attraction']).size().unstack(fill_value=0)

# Plot the heatmap
plt.figure(figsize=(16, 10))
sns.heatmap(attraction_country_counts, cmap="YlGnBu", annot=False, cbar=True)
plt.title('Heatmap of Top Attractions by Country')
plt.xlabel('Attraction')
plt.ylabel('Country')
plt.xticks(rotation=45)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming `final_merged_df` is the final dataframe containing the necessary columns

# Step: Boxplot of Ratings by Visit Mode and Attraction Type
plt.figure(figsize=(14, 8))
sns.boxplot(data=final_merged_df, x='VisitMode', y='Rating', hue='AttractionType')
plt.title('User Rating Patterns by Visit Mode and Attraction Type')
plt.xlabel('Visit Mode')
plt.ylabel('Rating')
plt.legend(title='Attraction Type', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from collections import Counter

# Assuming 'final_merged_df' is available and contains necessary columns: 'Cluster', 'Attraction', 'AttractionType', 'VisitMonth', 'VisitMode'

# Step 1: Prepare Data for Network Graph
# Sample popular attractions, visit modes, and months per cluster
cluster_attraction = final_merged_df.groupby(['Cluster', 'AttractionType', 'VisitMonth', 'VisitMode']).size().reset_index(name='Count')

# Only keep the top 5 attraction types, visit modes, and months per cluster for clarity
top_cluster_attraction = cluster_attraction.sort_values('Count', ascending=False).groupby('Cluster').head(5)

# Step 2: Create Network Graph
G = nx.Graph()

# Add user cluster nodes
clusters = top_cluster_attraction['Cluster'].unique()
for cluster in clusters:
    G.add_node(f"Cluster {cluster}", type="cluster", color="lightblue")

# Add attraction type nodes and connect to clusters
attractions = top_cluster_attraction['AttractionType'].unique()
for attraction in attractions:
    G.add_node(attraction, type="attraction", color="lightgreen")

# Add month nodes and connect to attraction types
months = top_cluster_attraction['VisitMonth'].unique()
for month in months:
    G.add_node(f"Month {month}", type="month", color="orange")

# Add visit mode nodes and connect to attraction types
visit_modes = top_cluster_attraction['VisitMode'].unique()
for mode in visit_modes:
    G.add_node(f"VisitMode {mode}", type="visitmode", color="purple")

# Add edges between clusters, attraction types, visit modes, and months based on top data
for _, row in top_cluster_attraction.iterrows():
    cluster_node = f"Cluster {row['Cluster']}"
    attraction_node = row['AttractionType']
    month_node = f"Month {row['VisitMonth']}"
    visit_mode_node = f"VisitMode {row['VisitMode']}"

    # Add edges
    G.add_edge(cluster_node, attraction_node)
    G.add_edge(attraction_node, month_node)
    G.add_edge(attraction_node, visit_mode_node)

# Step 3: Draw Network Graph with Colors
pos = nx.spring_layout(G, seed=42)  # Layout for the graph

# Color nodes based on type
node_colors = [G.nodes[node]['color'] for node in G.nodes]
node_labels = {node: node for node in G.nodes}

plt.figure(figsize=(14, 14))
nx.draw(G, pos, with_labels=True, labels=node_labels, node_color=node_colors, node_size=3000, font_size=10, font_color='black', font_weight='bold', edge_color='gray')
plt.title("Network Graph of User Clusters, Recommended Attractions, Visit Modes, and Months")
plt.show()

import pandas as pd

# Assuming `final_merged_df` contains columns for Country, VisitMode, VisitMonth, AttractionType, and AttractionPlace
# Example columns: ['UserId', 'Country', 'VisitMode', 'VisitMonth', 'AttractionType', 'Attraction', 'Rating']

# Step 1: Filter Top Attraction Types based on the input parameters
def get_top_attraction_types(country, visit_mode, visit_month, num_types=5):
    # Filter data based on user's country, visit mode, and visit month
    filtered_data = final_merged_df[
        (final_merged_df['Country'] == country) &
        (final_merged_df['VisitMode'] == visit_mode) &
        (final_merged_df['VisitMonth'] == visit_month)
    ]

    # Get the top attraction types by counting occurrences within the filtered data
    top_types = filtered_data['AttractionType'].value_counts().head(num_types)
    return top_types.index.tolist()

# Step 2: Recommend Top Attraction Places based on selected Attraction Type
def get_top_attraction_places(country, visit_mode, visit_month, attraction_type, num_places=5):
    # Filter data based on user's country, visit mode, visit month, and chosen attraction type
    filtered_data = final_merged_df[
        (final_merged_df['Country'] == country) &
        (final_merged_df['VisitMode'] == visit_mode) &
        (final_merged_df['VisitMonth'] == visit_month) &
        (final_merged_df['AttractionType'] == attraction_type)
    ]

    # Get the top attraction places by average rating within the filtered data
    top_places = (
        filtered_data.groupby('Attraction')['Rating']
        .mean()
        .sort_values(ascending=False)
        .head(num_places)
    )

    return top_places.index.tolist()

# Example Usage
user_country = "United Kingdom"      # Replace with user's country
user_visit_mode = "Family"           # Replace with user's visit mode (e.g., 'Family', 'Couples')
user_visit_month = 7                 # Replace with user's visit month (e.g., 7 for July)

# Step 1: Get Top Attraction Types
top_attraction_types = get_top_attraction_types(user_country, user_visit_mode, user_visit_month)
print(f"Top Attraction Types for {user_country} in {user_visit_month} (Visit Mode: {user_visit_mode}):")
for i, attraction_type in enumerate(top_attraction_types, 1):
    print(f"{i}. {attraction_type}")

# Example: Assuming the user selects an attraction type
selected_attraction_type = top_attraction_types[0]  # Let’s say user selects the first option

# Step 2: Get Top Attraction Places within the selected attraction type
top_attraction_places = get_top_attraction_places(
    user_country, user_visit_mode, user_visit_month, selected_attraction_type
)
print(f"\nTop Attraction Places for {selected_attraction_type}:")
for i, place in enumerate(top_attraction_places, 1):
    print(f"{i}. {place}")



# Remove rows with ratings less than 3 as they appear to be outliers
filtered_data = final_merged_df[final_merged_df['Rating'] >= 3]

# Proceed with the rest of the modeling process using filtered_data

"""# **Models**"""

'''
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report

# Load and preprocess the data
data = final_merged_df[['Country', 'VisitMode', 'VisitMonth', 'AttractionType', 'Rating']]

# Outlier removal based on Rating
data = data[data['Rating'] >= 3]

# Encoding categorical variables
label_encoders = {}
for column in ['Country', 'VisitMode', 'AttractionType']:
    le = LabelEncoder()
    data[column] = le.fit_transform(data[column])
    label_encoders[column] = le

# Creating cyclic features for VisitMonth
data['VisitMonth_sin'] = np.sin(2 * np.pi * data['VisitMonth'] / 12)
data['VisitMonth_cos'] = np.cos(2 * np.pi * data['VisitMonth'] / 12)

# Selecting features and target, dropping the 'Rating' column
X = data[['Country', 'VisitMode', 'VisitMonth_sin', 'VisitMonth_cos']]
y = data['AttractionType']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define models for comparison
models = {
    'RandomForest': RandomForestClassifier(random_state=42),
    'GradientBoosting': GradientBoostingClassifier(random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),
    'LogisticRegression': LogisticRegression(max_iter=1000),
    'KNN': KNeighborsClassifier()
}

# Hyperparameters for Grid Search
param_grids = {
    'RandomForest': {'n_estimators': [100, 200], 'max_depth': [10, 20]},
    'GradientBoosting': {'n_estimators': [100, 200], 'learning_rate': [0.05, 0.1], 'max_depth': [3, 5]},
    'XGBoost': {'n_estimators': [100, 200], 'learning_rate': [0.05, 0.1], 'max_depth': [3, 5]},
    'LogisticRegression': {'C': [0.1, 1, 10]},
    'KNN': {'n_neighbors': [5, 10]}
}

# Grid search to find the best model
best_models = {}
for model_name, model in models.items():
    print(f"Training {model_name}...")
    grid_search = GridSearchCV(model, param_grids[model_name], cv=5, scoring='accuracy', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_models[model_name] = grid_search.best_estimator_
    print(f"Best parameters for {model_name}: {grid_search.best_params_}")
    print(f"Best cross-validation accuracy for {model_name}: {grid_search.best_score_}\n")

# Select the best model based on cross-validation accuracy
best_model_name = max(best_models, key=lambda name: best_models[name].score(X_test, y_test))
best_model = best_models[best_model_name]
print(f"Selected model: {best_model_name}")

# Evaluate the best model
y_pred = best_model.predict(X_test)
print(f"Test Accuracy for {best_model_name}: {accuracy_score(y_test, y_pred)}")
print(classification_report(y_test, y_pred))

# Feature importance (if applicable)
if hasattr(best_model, "feature_importances_"):
    importances = best_model.feature_importances_
    feature_names = X.columns
    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': importances})
    print("Feature importances:")
    print(feature_importances.sort_values(by='importance', ascending=False))
'''

import tensorflow as tf
print("GPU is", "available" if tf.config.list_physical_devices('GPU') else "not available")

print(final_merged_df.head(5))

"""# **Correlation Analysis**"""

import seaborn as sns
import matplotlib.pyplot as plt

# Correlation matrix
plt.figure(figsize=(12, 8))
correlation_matrix = final_merged_df[['Continent_encoded', 'Country_encoded', 'VisitMode_encoded', 'VisitMonth_sin', 'VisitMonth_cos', 'AttractionType_encoded']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix for Encoded Features")
plt.show()

"""# **Distribution of Ratings by AttractionType and VisitMode**"""

plt.figure(figsize=(14, 6))
sns.boxplot(data=final_merged_df, x='AttractionType', y='Rating', hue='VisitMode')
plt.xticks(rotation=90)
plt.title("Distribution of Ratings by AttractionType and VisitMode")
plt.show()

"""# **Count of AttractionType by Country and VisitMode**"""

plt.figure(figsize=(14, 8))
sns.countplot(data=final_merged_df, x='AttractionType', hue='Country', order=final_merged_df['AttractionType'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Count of AttractionType by Country")
plt.show()

plt.figure(figsize=(14, 8))
sns.countplot(data=final_merged_df, x='AttractionType', hue='VisitMode', order=final_merged_df['AttractionType'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Count of AttractionType by VisitMode")
plt.show()

"""# **Seasonal Trends in AttractionType Visits**"""

plt.figure(figsize=(14, 8))
sns.lineplot(data=final_merged_df, x='VisitMonth', y='Rating', hue='AttractionType', ci=None)
plt.xticks(range(1, 13))
plt.title("Seasonal Trends in Ratings by AttractionType")
plt.show()

"""# **Cluster Analysis**"""

plt.figure(figsize=(10, 6))
sns.countplot(data=final_merged_df, x='Cluster', hue='AttractionType')
plt.title("Cluster Distribution by AttractionType")
plt.show()

# Clusters by Visit Mode
plt.figure(figsize=(10, 6))
sns.countplot(data=final_merged_df, x='Cluster', hue='VisitMode')
plt.title("Cluster Distribution by Visit Mode")
plt.show()

"""# **Feature Importance with a Random Forest Model (for initial insight)**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder

# Select relevant features for modeling
X = final_merged_df[['Continent_encoded', 'Country_encoded', 'VisitMode_encoded', 'VisitMonth_sin', 'VisitMonth_cos']]
y = final_merged_df['AttractionType_encoded']

# Train a simple random forest model
model = RandomForestClassifier(random_state=42)
model.fit(X, y)

# Plot feature importances
importances = model.feature_importances_
feature_names = X.columns
plt.figure(figsize=(10, 6))
sns.barplot(x=importances, y=feature_names)
plt.title("Feature Importance (Random Forest)")
plt.show()

"""# **Association Rules Analysis**"""

from mlxtend.frequent_patterns import apriori, association_rules

# Prepare data for association rules analysis
encoded_df = final_merged_df[['Country', 'VisitMode', 'AttractionType']].copy()
for column in encoded_df.columns:
    encoded_df[column] = encoded_df[column].astype('category')

# Convert data into one-hot encoding
onehot_df = pd.get_dummies(encoded_df)

# Apply Apriori
frequent_itemsets = apriori(onehot_df, min_support=0.01, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)
rules = rules.sort_values(by=['confidence', 'lift'], ascending=False).head(10)

# Plot top rules by confidence
plt.figure(figsize=(10, 6))
sns.barplot(x='confidence', y='antecedents', data=rules, orient='h')
plt.title("Top 10 Association Rules by Confidence")
plt.show()

"""# **Outlier Detection and Handling Code(New-Data)**"""

import pandas as pd
import numpy as np

# Make a copy of the original DataFrame
data = final_merged_df.copy()

# Outlier handling function using the IQR method for Rating column
def handle_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    # Remove rows with outliers in the column
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Apply outlier handling on the Rating column
data = handle_outliers(data, 'Rating')

# Check the distribution after removing outliers
print("Data shape after outlier handling:", data.shape)
print("Rating distribution after handling outliers:")
print(data['Rating'].describe())

"""# **Country-Specific Target Encoding**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Target encoding for Country based on average rating for each attraction type
country_target_encoding = data.groupby('Country')['AttractionType_encoded'].mean().to_dict()
data['Country_Target_Encoding'] = data['Country'].map(country_target_encoding)

"""# **Interaction Terms for Key Features**"""

# Interaction terms
data['Country_VisitMode'] = data['Country_encoded'] * data['VisitMode_encoded']
data['VisitMode_VisitMonth'] = data['VisitMode_encoded'] * data['VisitMonth']

"""# **Cyclic Encoding for VisitMonth with Seasonal Categories**"""

# Cyclic encoding is already done; add seasonal categories
data['Season'] = data['VisitMonth'].apply(lambda x: 'Peak' if x in [6, 7, 8] else 'Off-Peak' if x in [12, 1, 2] else 'Shoulder')
data['Season_Encoded'] = data['Season'].map({'Peak': 2, 'Shoulder': 1, 'Off-Peak': 0})

"""# **Cluster-Based Features**"""

from sklearn.cluster import KMeans

# Choose features for clustering
cluster_features = data[['Country_encoded', 'VisitMode_encoded', 'AttractionType_encoded']]

# Apply KMeans clustering
kmeans = KMeans(n_clusters=4, random_state=42)
data['Cluster_Label'] = kmeans.fit_predict(cluster_features)

"""# **Association Rule-Based Feature Engineering**"""

# Example association-based feature for family preference in Water Parks
data['Family_Preference'] = ((data['VisitMode'] == 'Family') & (data['AttractionType'] == 'Water Parks')).astype(int)

# Similarly, add more features based on other strong association rules
data['Country_Attraction_Score'] = data.apply(lambda row: 1 if row['Country'] == 'Australia' and row['AttractionType'] == 'Water Parks' else 0, axis=1)

"""# **Rating-Based Aggregates**"""

# Average and variance of ratings by attraction type and visit mode
rating_agg = data.groupby(['AttractionType_encoded', 'VisitMode_encoded'])['Rating'].agg(['mean', 'var']).reset_index()
rating_agg.columns = ['AttractionType_encoded', 'VisitMode_encoded', 'Avg_Rating', 'Rating_Variance']

# Merge these features back into the main dataset
data = pd.merge(data, rating_agg, on=['AttractionType_encoded', 'VisitMode_encoded'], how='left')

"""# **Model Training with Enhanced Features**

# **99% With extra-features**
"""

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# Define feature set and target
X = data[['Country_Target_Encoding', 'Country_VisitMode', 'VisitMode_VisitMonth',
          'VisitMonth_sin', 'VisitMonth_cos', 'Season_Encoded', 'Cluster_Label',
          'Family_Preference', 'Country_Attraction_Score', 'Avg_Rating', 'Rating_Variance']]
y = data['AttractionType_encoded']

# Train-test split (for final evaluation after cross-validation)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model
model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)

# Perform cross-validation
cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')  # 5-fold cross-validation
print("Cross-Validation Scores:", cv_scores)
print("Mean Cross-Validation Accuracy:", np.mean(cv_scores))

# Train the model on the entire training set
model.fit(X_train, y_train)

# Evaluate on the test set
test_accuracy = model.score(X_test, y_test)
print("Test Set Accuracy:", test_accuracy)

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier

# Assume `final_merged_df` is the dataset containing 'Attraction' column, which represents places.
data['AttractionPlace'] = final_merged_df['Attraction']  # Ensure 'Attraction' is the column with place names

# Encode 'AttractionPlace' as the target variable
le_attraction_place = LabelEncoder()
data['AttractionPlace_encoded'] = le_attraction_place.fit_transform(data['AttractionPlace'])

# Feature Engineering: Interaction terms and aggregated features
# Interaction between Country and AttractionType
data['Country_AttractionType'] = data['Country_encoded'] * data['AttractionType_encoded']

# Interaction between VisitMode and AttractionType
data['VisitMode_AttractionType'] = data['VisitMode_encoded'] * data['AttractionType_encoded']

# Interaction between Country and Season
data['Country_Season'] = data['Country_encoded'] * data['Season_Encoded']

# Include the rating from the merged dataset
data['Rating'] = final_merged_df['Rating']

# Aggregated Features based on historical data
# Attraction Popularity (average rating per AttractionType)
attraction_popularity = data.groupby('AttractionType_encoded')['Rating'].mean().to_dict()
data['Attraction_Popularity'] = data['AttractionType_encoded'].map(attraction_popularity)

# Country-Specific Attraction Popularity
country_attraction_rating = data.groupby(['Country_encoded', 'AttractionType_encoded'])['Rating'].mean().to_dict()
data['Country_Attraction_Rating'] = data.apply(lambda x: country_attraction_rating.get((x['Country_encoded'], x['AttractionType_encoded']), 0), axis=1)

# Clustering: Placeholder for potential clustering labels (dummy feature)
data['User_Cluster_Label'] = data['Country_encoded'] + data['VisitMode_encoded'] + data['AttractionType_encoded']  # Replace with actual clustering if available

"""# **final_merge_encoding**"""

data.to_csv("final_merge_encoding.csv", index=False)

print(data.head(5))
print(data.columns)

"""# **Model-1.1**

## **38% Model**
"""

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# Define revised feature set and target
X = data[['Country_Target_Encoding', 'Country_VisitMode', 'VisitMode_VisitMonth',
          'VisitMonth_sin', 'VisitMonth_cos', 'Season_Encoded',
          'Family_Preference', 'Country_Attraction_Score', 'Country_Season']]
y = data['AttractionType_encoded']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model
model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)

# Perform cross-validation
cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
print("Cross-Validation Scores:", cv_scores)
print("Mean Cross-Validation Accuracy:", np.mean(cv_scores))

# Train the model on the full training set
model.fit(X_train, y_train)

# Evaluate on the test set
test_accuracy = model.score(X_test, y_test)
print("Test Set Accuracy:", test_accuracy)

'''
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# Define revised feature set and target
X = data[['Country_Target_Encoding', 'Country_VisitMode', 'VisitMode_VisitMonth',
          'VisitMonth_sin', 'VisitMonth_cos', 'Season_Encoded',
          'Family_Preference', 'Country_Attraction_Score']]
y = data['AttractionType_encoded']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'RandomForest': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),
    'KNeighbors': KNeighborsClassifier(n_neighbors=5),
    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),
    'XGBoost': XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='mlogloss', random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)
}

# Perform cross-validation and evaluate each model
for model_name, model in models.items():
    print(f"Evaluating {model_name}...")

    # Cross-validation scores
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    print(f"{model_name} Cross-Validation Scores:", cv_scores)
    print(f"{model_name} Mean Cross-Validation Accuracy:", np.mean(cv_scores))

    # Train on full training set and evaluate on test set
    model.fit(X_train, y_train)
    test_accuracy = model.score(X_test, y_test)
    print(f"{model_name} Test Set Accuracy:", test_accuracy)
    print("="*40)
'''

"""# **Neural Networks**"""

'''
import numpy as np
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical

# Define feature set and target
X = data[['Country_Target_Encoding', 'Country_VisitMode', 'VisitMode_VisitMonth',
          'VisitMonth_sin', 'VisitMonth_cos', 'Season_Encoded',
          'Family_Preference', 'Country_Attraction_Score', 'Country_Season']].values
y = data['AttractionType_encoded'].values

# Scale the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# One-hot encode target for neural network
num_classes = len(np.unique(y))
y = to_categorical(y, num_classes)

# Define neural network model
def create_model():
    model = Sequential()
    model.add(Dense(64, input_dim=X.shape[1], activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(16, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))

    # Compile model
    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# K-Fold Cross Validation
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
fold_accuracies = []

for train_index, val_index in kfold.split(X):
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # Create and train model
    model = create_model()
    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0, validation_data=(X_val, y_val))

    # Evaluate model
    _, accuracy = model.evaluate(X_val, y_val, verbose=0)
    fold_accuracies.append(accuracy)
    print(f"Fold Accuracy: {accuracy}")

# Calculate and display average cross-validation accuracy
mean_accuracy = np.mean(fold_accuracies)
print("Mean Cross-Validation Accuracy:", mean_accuracy)

# Train on full training set and evaluate on test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
final_model = create_model()
final_model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, validation_data=(X_test, y_test))

# Test set evaluation
_, test_accuracy = final_model.evaluate(X_test, y_test)
print("Test Set Accuracy:", test_accuracy)
'''

"""# **Other Models**"""

'''
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
import numpy as np

# Define revised feature set and target
X = data[['Country_Target_Encoding', 'Country_VisitMode', 'VisitMode_VisitMonth',
          'VisitMonth_sin', 'VisitMonth_cos', 'Season_Encoded',
          'Family_Preference', 'Country_Attraction_Score', 'Country_Season']]
y = data['AttractionType_encoded']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'RandomForest': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),
    'KNeighbors': KNeighborsClassifier(n_neighbors=5),
    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),
    'XGBoost': XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='mlogloss', random_state=42),
    'SVM': SVC(kernel='rbf', gamma='scale', C=1, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)
}

# Dictionary to store results
results = {}

# Perform cross-validation and evaluate each model
for model_name, model in models.items():
    print(f"Evaluating {model_name}...")

    # Cross-validation scores
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    cv_mean_accuracy = np.mean(cv_scores)
    print(f"{model_name} Cross-Validation Scores:", cv_scores)
    print(f"{model_name} Mean Cross-Validation Accuracy:", cv_mean_accuracy)

    # Train on full training set and evaluate on test set
    model.fit(X_train, y_train)
    test_accuracy = model.score(X_test, y_test)
    print(f"{model_name} Test Set Accuracy:", test_accuracy)
    print("="*40)

    # Store results
    results[model_name] = {
        'cross_val_scores': cv_scores,
        'mean_cross_val_accuracy': cv_mean_accuracy,
        'test_accuracy': test_accuracy
    }

# Identify the best-performing model based on test set accuracy
best_model_name = max(results, key=lambda x: results[x]['test_accuracy'])
print(f"Best performing model: {best_model_name}")
print(f"Cross-Validation Mean Accuracy: {results[best_model_name]['mean_cross_val_accuracy']}")
print(f"Test Set Accuracy: {results[best_model_name]['test_accuracy']}")
'''

"""# **Label Encoders**

**PREDICTION MODEL 1.1**
"""

# Step 1: Encode categorical features
label_encoders = {}
for column in ['Country', 'VisitMode', 'AttractionType']:
    le = LabelEncoder()
    data[f"{column}_encoded"] = le.fit_transform(data[column])
    label_encoders[column] = le

import pandas as pd
import numpy as np


# Example mappings for features (ensure these mappings are precomputed and stored)
country_visitmode_mapping = data.set_index(['Country_encoded', 'VisitMode_encoded'])['Country_VisitMode'].to_dict()
family_preference_mapping = data.set_index('Country_encoded')['Family_Preference'].to_dict()
country_attraction_score_mapping = data.set_index('Country_encoded')['Country_Attraction_Score'].to_dict()

def preprocess_for_prediction(country, visit_mode, visit_month):
    # Encode country and visit_mode using LabelEncoder
    country_target_encoding = label_encoders['Country'].transform([country])[0]
    visit_mode_encoded = label_encoders['VisitMode'].transform([visit_mode])[0]

    # Interaction term: Country_VisitMode
    country_visitmode = country_visitmode_mapping.get((country, visit_mode), 0)

    # VisitMode and VisitMonth interaction term
    visitmode_visitmonth = visit_mode_encoded * visit_month

    # Family preference
    family_preference = family_preference_mapping.get(country, 0)

    # Country-specific attraction score
    country_attraction_score = country_attraction_score_mapping.get(country, 0)

    # Seasonal encoding based on VisitMonth (0: Winter, 1: Spring, 2: Summer, 3: Fall)
    season_encoded = (visit_month % 12) // 3

    # Interaction term: Country and Season
    country_season = country_target_encoding * season_encoded

    # Cyclic encoding for VisitMonth
    visit_month_sin = np.sin(2 * np.pi * visit_month / 12)
    visit_month_cos = np.cos(2 * np.pi * visit_month / 12)

    # Create DataFrame for prediction input (only include features used in training)
    input_data = pd.DataFrame({
        'Country_Target_Encoding': [country_target_encoding],
        'Country_VisitMode': [country_visitmode],
        'VisitMode_VisitMonth': [visitmode_visitmonth],
        'VisitMonth_sin': [visit_month_sin],
        'VisitMonth_cos': [visit_month_cos],
        'Season_Encoded': [season_encoded],
        'Family_Preference': [family_preference],
        'Country_Attraction_Score': [country_attraction_score],
        'Country_Season': [country_season]
    })

    return input_data

# Example usage
country_input = 'United Kingdom'
visit_mode_input = 'Friends'
visit_month_input = 6

# Preprocess the input
input_data = preprocess_for_prediction(country_input, visit_mode_input, visit_month_input)
print(input_data)

# Make prediction
predicted_attraction_type = model.predict(input_data)
predicted_attraction_type_name = label_encoders['AttractionType'].inverse_transform(predicted_attraction_type)
print("Predicted Attraction Type:", predicted_attraction_type_name[0])

'''
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier

# Step 1: Encode categorical features
label_encoders = {}
for column in ['Country', 'VisitMode', 'AttractionType']:
    le = LabelEncoder()
    data[f"{column}_encoded"] = le.fit_transform(data[column])
    label_encoders[column] = le

# Example association-based features for Family Preference and Country Attraction Score
data['Family_Preference'] = ((data['VisitMode'] == 'Family') & (data['AttractionType'] == 'Water Parks')).astype(int)
data['Country_Attraction_Score'] = data.apply(lambda row: 1 if row['Country'] == 'Australia' and row['AttractionType'] == 'Water Parks' else 0, axis=1)

# Interaction terms
data['Country_VisitMode'] = data['Country_encoded'] * data['VisitMode_encoded']
data['VisitMode_VisitMonth'] = data['VisitMode_encoded'] * data['VisitMonth']

# Define mappings to be used in prediction
country_visitmode_mapping = data.set_index(['Country', 'VisitMode'])['Country_VisitMode'].to_dict()
family_preference_mapping = data.set_index('Country')['Family_Preference'].to_dict()
country_attraction_score_mapping = data.set_index('Country')['Country_Attraction_Score'].to_dict()

# Fit model (for illustration purposes)
X = data[['Country_VisitMode', 'VisitMode_VisitMonth', 'Family_Preference', 'Country_Attraction_Score']]
y = data['AttractionType_encoded']
model = RandomForestClassifier(random_state=42)
model.fit(X, y)

# Prediction preprocessing function
def preprocess_for_prediction(country, visit_mode, visit_month):
    # Encode country and visit_mode using LabelEncoder
    country_target_encoding = label_encoders['Country'].transform([country])[0]
    visit_mode_encoded = label_encoders['VisitMode'].transform([visit_mode])[0]

    # Interaction term: Country_VisitMode
    country_visitmode = country_visitmode_mapping.get((country, visit_mode), 0)

    # VisitMode and VisitMonth interaction term
    visitmode_visitmonth = visit_mode_encoded * visit_month

    # Family preference
    family_preference = family_preference_mapping.get(country, 0)

    # Country-specific attraction score
    country_attraction_score = country_attraction_score_mapping.get(country, 0)

    # Create DataFrame for prediction input (only include features used in training)
    input_data = pd.DataFrame({
        'Country_VisitMode': [country_visitmode],
        'VisitMode_VisitMonth': [visitmode_visitmonth],
        'Family_Preference': [family_preference],
        'Country_Attraction_Score': [country_attraction_score]
    })

    return input_data

# Example usage
country_input = 'United Kingdom'
visit_mode_input = 'Friends'
visit_month_input = 6

# Preprocess the input
input_data = preprocess_for_prediction(country_input, visit_mode_input, visit_month_input)
print(input_data)
# Make prediction
predicted_attraction_type = model.predict(input_data)
predicted_attraction_type_name = label_encoders['AttractionType'].inverse_transform(predicted_attraction_type)
print("Predicted Attraction Type:", predicted_attraction_type_name[0])

/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.
  and should_run_async(code)
   Country_VisitMode  VisitMode_VisitMonth  Family_Preference  \
0                  4                    12                  0

   Country_Attraction_Score
0                         0
Predicted Attraction Type: Beaches

'''

# data = pd.read_csv("/content/final_merge_encoding.csv")

# print(data.head(5))

"""# **Model-1.2**"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier

'''
# Assume `final_merged_df` is the dataset containing 'Attraction' column, which represents places.
data['AttractionPlace'] = final_merged_df['Attraction']  # Ensure 'Attraction' is the column with place names

# Encode 'AttractionPlace' as the target variable
le_attraction_place = LabelEncoder()
data['AttractionPlace_encoded'] = le_attraction_place.fit_transform(data['AttractionPlace'])

# Feature Engineering: Interaction terms and aggregated features
# Interaction between Country and AttractionType
data['Country_AttractionType'] = data['Country_encoded'] * data['AttractionType_encoded']

# Interaction between VisitMode and AttractionType
data['VisitMode_AttractionType'] = data['VisitMode_encoded'] * data['AttractionType_encoded']

# Interaction between Country and Season
data['Country_Season'] = data['Country_encoded'] * data['Season_Encoded']

# Include the rating from the merged dataset
data['Rating'] = final_merged_df['Rating']

# Aggregated Features based on historical data
# Attraction Popularity (average rating per AttractionType)
attraction_popularity = data.groupby('AttractionType_encoded')['Rating'].mean().to_dict()
data['Attraction_Popularity'] = data['AttractionType_encoded'].map(attraction_popularity)

# Country-Specific Attraction Popularity
country_attraction_rating = data.groupby(['Country_encoded', 'AttractionType_encoded'])['Rating'].mean().to_dict()
data['Country_Attraction_Rating'] = data.apply(lambda x: country_attraction_rating.get((x['Country_encoded'], x['AttractionType_encoded']), 0), axis=1)

# Clustering: Placeholder for potential clustering labels (dummy feature)
data['User_Cluster_Label'] = data['Country_encoded'] + data['VisitMode_encoded'] + data['AttractionType_encoded']  # Replace with actual clustering if available
'''

# Define feature set and target after including new engineered features
X = data[['Country_Target_Encoding', 'Country_VisitMode', 'VisitMode_VisitMonth',
          'VisitMonth_sin', 'VisitMonth_cos', 'Season_Encoded', 'Country_AttractionType',
          'VisitMode_AttractionType', 'Country_Season', 'Attraction_Popularity',
          'Country_Attraction_Rating']]
y = data['AttractionPlace_encoded']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the RandomForest model
model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)

# Perform cross-validation
cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
print("Cross-Validation Scores:", cv_scores)
print("Mean Cross-Validation Accuracy:", np.mean(cv_scores))

# Train the model on the full training set
model.fit(X_train, y_train)

# Evaluate on the test set
test_accuracy = model.score(X_test, y_test)
print("Test Set Accuracy:", test_accuracy)

"""# **PREDICTION MODEL1.2**

"""

import pandas as pd
import numpy as np

# Example mappings for features (Ensure these mappings are precomputed and stored)
country_visitmode_mapping = data.set_index(['Country_encoded', 'VisitMode_encoded'])['Country_VisitMode'].to_dict()
attraction_popularity = data.groupby('AttractionType_encoded')['Rating'].mean().to_dict()
country_attraction_rating = data.groupby(['Country_encoded', 'AttractionType_encoded'])['Rating'].mean().to_dict()

def predict_top_attractions(input_country, input_visit_mode, input_visit_month, input_attraction_type):
    # Encode inputs based on existing LabelEncoders
    country_encoded = label_encoders['Country'].transform([input_country])[0]
    visit_mode_encoded = label_encoders['VisitMode'].transform([input_visit_mode])[0]
    attraction_type_encoded = label_encoders['AttractionType'].transform([input_attraction_type])[0]

    # Calculate interaction features and other engineered features
    country_attraction_type = country_encoded * attraction_type_encoded
    visitmode_attraction_type = visit_mode_encoded * attraction_type_encoded
    country_season = country_encoded * (input_visit_month % 12 // 3)

    # Cyclic encoding for VisitMonth
    visit_month_sin = np.sin(2 * np.pi * input_visit_month / 12)
    visit_month_cos = np.cos(2 * np.pi * input_visit_month / 12)

    # Prepare input for prediction
    input_data = pd.DataFrame({
        'Country_Target_Encoding': [country_encoded],
        'Country_VisitMode': [country_encoded * visit_mode_encoded],
        'VisitMode_VisitMonth': [visit_mode_encoded * input_visit_month],
        'VisitMonth_sin': [visit_month_sin],
        'VisitMonth_cos': [visit_month_cos],
        'Season_Encoded': [input_visit_month % 12 // 3],
        'Country_AttractionType': [country_attraction_type],
        'VisitMode_AttractionType': [visitmode_attraction_type],
        'Country_Season': [country_season],
        'Attraction_Popularity': [attraction_popularity.get(attraction_type_encoded, 0)],
        'Country_Attraction_Rating': [country_attraction_rating.get((country_encoded, attraction_type_encoded), 0)]
    })

    # Predict top attraction places
    top_attractions_proba = model.predict_proba(input_data)[0]
    top_5_indices = np.argsort(top_attractions_proba)[-5:][::-1]
    top_5_attractions = le_attraction_place.inverse_transform(top_5_indices)

    return top_5_attractions

# Example usage

# Top 5 Recommended Attractions: ['Kuta Beach - Bali' 'Malioboro Road' 'Merapi Volcano' 'Tanah Lot Temple' 'Seminyak Beach']

predicted_attractions = predict_top_attractions('Australia', 'Family', 6, 'Water Parks')
print("Top 5 Recommended Attractions:", predicted_attractions)

import pickle

# Save the LabelEncoder to a pickle file
with open('le_attraction_place.pkl', 'wb') as file:
    pickle.dump(le_attraction_place, file)

print("LabelEncoder for 'AttractionPlace' saved successfully.")

"""# **Model-1.1**"""

data.shape



"""# **Model 3.1**"""

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold

# Load dataset
# (Replace 'dataset_path.csv' with the actual path to your CSV dataset)
dataset = pd.read_csv('final_merged_tourism_data.csv')

# Extract relevant columns
data = dataset[['Country', 'VisitMode', 'AttractionType', 'AttractionAddress']]

# Encode categorical variables
label_encoders = {}
for column in ['Country', 'VisitMode', 'AttractionType', 'AttractionAddress']:
    le = LabelEncoder()
    data[column] = le.fit_transform(data[column])
    label_encoders[column] = le

# Split data into features and target
X = data[['Country', 'VisitMode', 'AttractionType']]
y = data['AttractionAddress']

# Initialize a RandomForestClassifier as a base model
model = RandomForestClassifier()

# 4-fold cross-validation
kf = KFold(n_splits=4, shuffle=True, random_state=1)
cross_val_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')

# Train the model and evaluate accuracy
model.fit(X, y)
y_pred = model.predict(X)
accuracy = accuracy_score(y, y_pred)

print("Cross-Validation Accuracy Scores:", cross_val_scores)
print("Mean Cross-Validation Accuracy:", cross_val_scores.mean())
print("Model Accuracy on Training Data:", accuracy)

final_df = pd.read_csv("/content/final_merged_tourism_data.csv")

import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error
from sklearn.neighbors import NearestNeighbors
from collections import Counter
import numpy as np

# Load data
data = pd.read_csv('final_merged_tourism_data.csv')

# Preprocessing - Encode categorical variables
data['Country'] = data['Country'].astype('category').cat.codes
data['VisitMode'] = data['VisitMode'].astype('category').cat.codes
data['AttractionType'] = data['AttractionType'].astype('category').cat.codes

# Define a function to recommend attractions based on collaborative filtering
def recommend_attractions(input_country, input_mode, input_month, data, top_n=5):
    # Filter the data based on input parameters
    filtered_data = data[(data['Country'] == input_country) &
                         (data['VisitMode'] == input_mode) &
                         (data['VisitMonth'] == input_month)]

    if filtered_data.empty:
        return "No recommendations available for the selected inputs."

    # Group by attraction type and calculate mean rating, then sort to get top N attractions
    top_attractions = (filtered_data.groupby('AttractionType')['Rating']
                       .mean()
                       .sort_values(ascending=False)
                       .head(top_n))

    return top_attractions.index.tolist()

# Perform cross-validation
kf = KFold(n_splits=4, shuffle=True, random_state=42)
rmse_scores = []

# Encode unique combination of input attributes to treat as unique "users"
data['UserProfile'] = data.apply(lambda row: f"{row['Country']}_{row['VisitMode']}_{row['VisitMonth']}", axis=1)
user_profiles = data['UserProfile'].unique()

for train_index, test_index in kf.split(user_profiles):
    # Split data based on train/test user profiles
    train_profiles, test_profiles = user_profiles[train_index], user_profiles[test_index]
    train_data = data[data['UserProfile'].isin(train_profiles)]
    test_data = data[data['UserProfile'].isin(test_profiles)]

    # Fit collaborative filtering model on train data
    train_matrix = train_data.pivot(index='UserProfile', columns='AttractionType', values='Rating').fillna(0)
    model = NearestNeighbors(metric='cosine', algorithm='auto')
    model.fit(train_matrix)

    # Predict for test set
    predictions, actuals = [], []

    for profile in test_profiles:
        profile_data = test_data[test_data['UserProfile'] == profile]
        if not profile_data.empty:
            input_country = profile_data['Country'].iloc[0]
            input_mode = profile_data['VisitMode'].iloc[0]
            input_month = profile_data['VisitMonth'].iloc[0]

            # Get top 5 recommendations
            recommended_attractions = recommend_attractions(input_country, input_mode, input_month, train_data, top_n=5)

            # Actual top-rated attractions from the test set for this user profile
            actual_attractions = profile_data.sort_values(by='Rating', ascending=False)['AttractionType'].tolist()[:5]

            # Calculate RMSE based on presence of recommended attractions in actual top list
            intersection = len(set(recommended_attractions).intersection(set(actual_attractions)))
            score = mean_squared_error([1] * 5, [1 if i < intersection else 0 for i in range(5)])
            rmse_scores.append(np.sqrt(score))

# Display cross-validation results
print(f"Cross-Validation RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}")

# Example Usage
input_country = data['Country'].astype('category').cat.categories.get_loc('United Kingdom')
input_mode = data['VisitMode'].astype('category').cat.categories.get_loc('Couples')
input_month = 10

recommendations = recommend_attractions(input_country, input_mode, input_month, data, top_n=5)
print("Top 5 Recommended Attraction Types:", recommendations)