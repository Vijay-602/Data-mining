# -*- coding: utf-8 -*-
"""Mining-Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ib9YF9MkovmGRpPmSfYF52aSdKVMEmLC
"""

#!pip install mlxtend==0.19.0

import pandas as pd
import numpy as np
file_df = pd.read_csv('Integrated_data.csv')

"""*Checking for missing values*"""

# Check for missing values in the entire DataFrame
missing_values = file_df.isnull().sum()

# Display columns with missing values and their count
missing_columns = missing_values[missing_values > 0]
print("Columns with missing values:\n", missing_columns)

# Alternatively, display a summary of missing values
print("\nTotal missing values in the dataset:", file_df.isnull().sum().sum())

"""*checked data-type of the columns in data*"""

# Display the data types of all columns
print(file_df.dtypes)

"""*observed object data-type instead of int64 for VisitMonth*"""

# Function to check if a value is not an integer
def find_non_integers(column):
    non_int_values = file_df[~file_df[column].astype(str).str.isdigit()]
    return non_int_values[[column]]

# Check for non-integer values in each specified column
for col in ['VisitMonth']:
    non_integers = find_non_integers(col)
    if not non_integers.empty:
        print(f"\nNon-integer values in column '{col}':")
        print(non_integers)
    else:
        print(f"\nAll values in column '{col}' are integers.")

"""*checked for non-integer unique entries*"""

# Function to find unique non-numeric values in a column
def find_non_numeric_values(column):
    non_numeric_values = file_df[~file_df[column].astype(str).str.isdigit()][column].unique()
    return non_numeric_values

# Check for non-numeric unique values in each specified column
for col in ['VisitMonth']:
    non_numeric_values = find_non_numeric_values(col)
    if len(non_numeric_values) > 0:
        print(f"\nUnique non-numeric values in column '{col}': {non_numeric_values}")
    else:
        print(f"\nAll values in column '{col}' are numeric.")

"""*changed the month from string version to integer version*"""

# Define a dictionary to map month names to their corresponding numbers
month_mapping = {
    'January': 1, 'February': 2, 'March': 3, 'April': 4,
    'May': 5, 'June': 6, 'July': 7, 'August': 8,
    'September': 9, 'October': 10, 'November': 11, 'December': 12
}

# Convert month names to their corresponding numbers
file_df['VisitMonth'] = file_df['VisitMonth'].replace(month_mapping)

# Check for non-numeric values after conversion
non_numeric_months = file_df[~file_df['VisitMonth'].astype(str).str.isdigit()]
if not non_numeric_months.empty:
    print("\nNon-numeric values remaining in 'VisitMonth':")
    print(non_numeric_months['VisitMonth'].unique())
else:
    print("\nAll values in 'VisitMonth' are now numeric.")
print()

"""*plotted the numeric values against it's frequency and found out-of range entries*"""

import matplotlib.pyplot as plt
import pandas as pd

# Print the number of NaN values in the 'VisitMonth' column before any changes
# nan_count_before = file_df['VisitMonth'].isna().sum()
# print(f"Number of NaN values in 'VisitMonth' before processing: {nan_count_before}")

# Convert non-null entries in 'VisitMonth' to strings without changing NaNs
file_df['VisitMonth'] = file_df['VisitMonth'].apply(lambda x: str(x) if pd.notna(x) else x)

# Filter out NaNs first, then check if the remaining entries are numeric
numeric_visit_month = file_df[file_df['VisitMonth'].notna() & file_df['VisitMonth'].str.isdigit()]['VisitMonth'].astype(int)

# Print the number of NaN values in the 'VisitMonth' column after processing
# nan_count_after = file_df['VisitMonth'].isna().sum()
# print(f"Number of NaN values in 'VisitMonth' after processing: {nan_count_after}")

# Plotting the frequency of numeric entries
plt.figure(figsize=(10, 6))
numeric_visit_month.value_counts().sort_index().plot(kind='bar')
plt.title('Frequency of Numeric Entries in VisitMonth')
plt.xlabel('Visit Month')
plt.ylabel('Frequency')
plt.xticks(rotation=0)
plt.grid(axis='y')
plt.show()

"""made sure NaN entries did not disappear in previous step"""

# Check for missing values in the entire DataFrame
missing_values = file_df.isnull().sum()

# Display columns with missing values and their count
missing_columns = missing_values[missing_values > 0]
print("Columns with missing values:\n", missing_columns)

# Alternatively, display a summary of missing values
print("\nTotal missing values in the dataset:", file_df.isnull().sum().sum())

"""*checked the data types again*"""

# Convert the column back to integer type where possible, keeping NaNs intact
file_df['VisitMonth'] = pd.to_numeric(file_df['VisitMonth'], errors='coerce').astype('Int64')

# Print the data types to verify
print("\nData types after conversion:")
print(file_df.dtypes)

"""*removed data with out-of range entries*"""

# Print the initial count of rows
initial_count = len(file_df)
print(f"Initial number of rows: {initial_count}")

# Remove rows where 'VisitMonth' is outside the range 1 to 12 (inclusive)
file_df = file_df[(file_df['VisitMonth'].isna()) | (file_df['VisitMonth'].between(1, 12))]

# Print the count of rows after removal
final_count = len(file_df)
print(f"Number of rows after removing out-of-range months: {final_count}")
print(f"Rows removed: {initial_count - final_count}")

# Display the resulting DataFrame (first 10 rows for verification)
print("\nSample data after cleaning:")
print(file_df.head(10))

"""*made sure the entries are removed*"""

# Display unique entries in sorted order, ignoring NaN
unique_months_sorted = sorted(file_df['VisitMonth'].dropna().unique())
print("Unique entries in 'VisitMonth' (sorted, excluding NaN):")
print(unique_months_sorted)

"""*counted the NaN values*"""

# Count the number of NaN values in the 'VisitMonth' column
nan_count = file_df['VisitMonth'].isna().sum()
print(f"Number of NaN values in 'VisitMonth': {nan_count}")

"""*removed the NaN data entries of VisitMonth column*"""

# Print the initial number of rows
initial_count = len(file_df)
print(f"Initial number of rows: {initial_count}")

# Remove rows where 'VisitMonth' is NaN
file_df = file_df.dropna(subset=['VisitMonth'])

# Print the number of rows after removing rows with NaN in 'VisitMonth'
final_count = len(file_df)
print(f"Number of rows after removing NaN in 'VisitMonth': {final_count}")
print(f"Rows removed: {initial_count - final_count}")

# Display the first few rows to confirm the changes
print("\nSample data after removing rows with NaN in 'VisitMonth':")
print(file_df.head(10))

"""*found the mapping between Attraction and AttractionType except NaN in AttractionType*"""

import pandas as pd

# Drop rows with NaN values in either 'AttractionType' or 'Attraction'
filtered_df = file_df.dropna(subset=['AttractionType', 'Attraction'])

# Remove duplicates to ensure unique mappings
unique_mappings = filtered_df[['AttractionType', 'Attraction']].drop_duplicates()

# Construct a dictionary with 'AttractionType' as keys and list of 'Attraction' as values
attraction_map = unique_mappings.groupby('AttractionType')['Attraction'].apply(list).to_dict()

# Display the resulting dictionary
for attraction_type, places in attraction_map.items():
    print(f"{attraction_type}: {places}\n")

file_df[['AttractionType', 'Attraction']].head(40)

"""*replaced the NaN entries with their respective AttractionType mapping from above code*"""

# Reverse the map to create a lookup by 'Attraction' to 'AttractionType'
attraction_to_type = {attraction: attraction_type for attraction_type, attractions in attraction_map.items() for attraction in attractions}

# Fill NaN values in 'AttractionType' based on the 'Attraction' column
file_df['AttractionType'] = file_df.apply(
    lambda row: attraction_to_type.get(row['Attraction'], row['AttractionType']) if pd.isna(row['AttractionType']) else row['AttractionType'],
    axis=1
)

# Display the DataFrame to verify the changes
print(file_df[['AttractionType', 'Attraction']].head(40))  # Display the first 20 rows for verification

file_df.to_csv("final_mining.csv")

"""**#######**"""

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/final_mining.csv')

"""**FINDING DUPLICATES**"""

import pandas as pd


# Check for duplicates
print("Number of duplicate rows before removal:", df.duplicated().sum())

# Display a few examples of duplicate rows, if any
duplicates = df[df.duplicated()]
print("\nSample of duplicate rows:")
print(duplicates.head())

"""**REMOVE DUPLICATES**"""

# Remove duplicate rows
df.drop_duplicates(inplace=True)

# Verify if duplicates have been removed
print("\nNumber of duplicate rows after removal:", df.duplicated().sum())

# Display the shape of the dataset after removing duplicates
print("\nShape of the dataset after removing duplicates:", df.shape)

"""**OUTLIER DETECTION**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set the style for seaborn plots
sns.set(style="whitegrid")

# Plot boxplot for 'VisitMonth'
plt.figure(figsize=(8, 6))
sns.boxplot(x=df['VisitMonth'])
plt.title("Box Plot for VisitMonth")
plt.xlabel("VisitMonth")
plt.show()

# Plot boxplot for 'Rating'
plt.figure(figsize=(8, 6))
sns.boxplot(x=df['Rating'])
plt.title("Box Plot for Rating")
plt.xlabel("Rating")
plt.show()

# Plot histogram for 'VisitMonth'
plt.figure(figsize=(8, 6))
plt.hist(df['VisitMonth'], bins=12, color='skyblue', edgecolor='black')
plt.title("Histogram of VisitMonth")
plt.xlabel("VisitMonth")
plt.ylabel("Frequency")
plt.show()

# Plot histogram for 'Rating'
plt.figure(figsize=(8, 6))
plt.hist(df['Rating'], bins=5, color='lightgreen', edgecolor='black')
plt.title("Histogram of Ratings")
plt.xlabel("Rating")
plt.ylabel("Frequency")
plt.show()

# Scatter plot to detect outliers between VisitMonth and Rating
plt.figure(figsize=(10, 6))
plt.scatter(df['VisitMonth'], df['Rating'], alpha=0.6, edgecolor='black')
plt.title("Scatter Plot: VisitMonth vs. Rating")
plt.xlabel("VisitMonth")
plt.ylabel("Rating")
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Set the style for seaborn plots
sns.set(style="whitegrid")

# Box plot: Rating by Country
plt.figure(figsize=(12, 6))
sns.boxplot(x='Country', y='Rating', data=df)
plt.xticks(rotation=45)
plt.title("Box Plot of Rating by Country")
plt.xlabel("Country")
plt.ylabel("Rating")
plt.show()

# Box plot: Rating by VisitMode
plt.figure(figsize=(10, 6))
sns.boxplot(x='VisitMode', y='Rating', data=df)
plt.title("Box Plot of Rating by Visit Mode")
plt.xlabel("Visit Mode")
plt.ylabel("Rating")
plt.show()

# Box plot: Rating by AttractionType
plt.figure(figsize=(12, 6))
sns.boxplot(x='AttractionType', y='Rating', data=df)
plt.xticks(rotation=45)
plt.title("Box Plot of Rating by Attraction Type")
plt.xlabel("Attraction Type")
plt.ylabel("Rating")
plt.show()

# Box plot: Rating by VisitMonth
plt.figure(figsize=(10, 6))
sns.boxplot(x='VisitMonth', y='Rating', data=df)
plt.title("Box Plot of Rating by Visit Month")
plt.xlabel("Visit Month")
plt.ylabel("Rating")
plt.show()

"""**Investigate Outliers:**
*   For entries with ratings of 1 or 2, investigate if there are patterns


"""

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/final_mining.csv')

# Filter rows with low ratings (1 or 2)
outliers_df = df[df['Rating'] <= 2]
print(f"Number of low rating outliers: {len(outliers_df)}")
print("\nSample of Outliers:")
print(outliers_df.head())

"""*Analysis by Country*"""

# Count the number of low ratings per country
country_outliers = outliers_df['Country'].value_counts()
print("\nLow Rating Count by Country:")
print(country_outliers.head(10))

# Visualize the distribution
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
country_outliers.head(10).plot(kind='bar', color='skyblue')
plt.title('Top 10 Countries with Low Ratings')
plt.xlabel('Country')
plt.ylabel('Number of Low Ratings')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

"""*Analysis by Visit Mode*"""

# Count the number of low ratings per visit mode
visit_mode_outliers = outliers_df['VisitMode'].value_counts()
print("\nLow Rating Count by Visit Mode:")
print(visit_mode_outliers)

# Visualize the distribution
plt.figure(figsize=(8, 5))
visit_mode_outliers.plot(kind='bar', color='orange')
plt.title('Low Ratings by Visit Mode')
plt.xlabel('Visit Mode')
plt.ylabel('Number of Low Ratings')
plt.grid(True)
plt.show()

"""*Analysis by Attraction Type*"""

# Count the number of low ratings per attraction type
attraction_outliers = outliers_df['AttractionType'].value_counts()
print("\nLow Rating Count by Attraction Type:")
print(attraction_outliers)

# Visualize the distribution
plt.figure(figsize=(10, 6))
attraction_outliers.plot(kind='bar', color='green')
plt.title('Low Ratings by Attraction Type')
plt.xlabel('Attraction Type')
plt.ylabel('Number of Low Ratings')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

"""*Analysis by Visit Month*"""

# Count the number of low ratings per visit month
month_outliers = outliers_df['VisitMonth'].value_counts().sort_index()
print("\nLow Rating Count by Visit Month:")
print(month_outliers)

# Visualize the distribution
plt.figure(figsize=(8, 5))
month_outliers.plot(kind='bar', color='purple')
plt.title('Low Ratings by Visit Month')
plt.xlabel('Visit Month')
plt.ylabel('Number of Low Ratings')
plt.grid(True)
plt.show()

"""*Multi-variable Analysis*"""

# Group by multiple columns to identify patterns
pattern_analysis = outliers_df.groupby(['Country', 'VisitMode', 'AttractionType']).size().reset_index(name='LowRatingCount')
pattern_analysis = pattern_analysis.sort_values(by='LowRatingCount', ascending=False)

print("\nTop Patterns for Low Ratings:")
print(pattern_analysis.head(10))

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('/content/final_mining.csv')

# Step 1: Separate the data into low ratings (1-2) and higher ratings (>2)
low_ratings_df = df[df['Rating'] <= 2]
high_ratings_df = df[df['Rating'] > 2]

# Step 2: Define a function to plot the comparison for each visit mode
def plot_attraction_ratings_comparison(visit_mode):
    # Filter data by visit mode
    low_ratings_mode = low_ratings_df[low_ratings_df['VisitMode'] == visit_mode]
    high_ratings_mode = high_ratings_df[high_ratings_df['VisitMode'] == visit_mode]

    # Count the number of ratings for each attraction type
    low_count = low_ratings_mode['AttractionType'].value_counts()
    high_count = high_ratings_mode['AttractionType'].value_counts()

    # Merge the counts into a single DataFrame for comparison
    comparison_df = pd.DataFrame({'Low Ratings (1-2)': low_count, 'High Ratings (>2)': high_count}).fillna(0)

    # Plot the comparison
    plt.figure(figsize=(12, 6))
    comparison_df.plot(kind='bar', stacked=False, color=['red', 'green'], edgecolor='black')
    plt.title(f"Attraction Type Ratings Comparison for Visit Mode: {visit_mode}")
    plt.xlabel('Attraction Type')
    plt.ylabel('Number of Ratings')
    plt.xticks(rotation=45, ha='right')
    plt.grid(axis='y')
    plt.legend(loc='upper right')
    plt.show()

# Step 3: Loop through each visit mode and plot the graphs
unique_visit_modes = df['VisitMode'].unique()
for mode in unique_visit_modes:
    plot_attraction_ratings_comparison(mode)

"""*clearly it seems there is no pattern*"""

import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('/content/final_mining.csv')

# Step 1: Separate the data into low ratings (1-2) and higher ratings (>2)
low_ratings_df = df[df['Rating'] <= 2]
high_ratings_df = df[df['Rating'] > 2]

# Step 2: Define a function to plot the comparison for each attraction type
def plot_monthly_ratings_comparison(attraction_type):
    # Filter data by attraction type
    low_ratings_attraction = low_ratings_df[low_ratings_df['AttractionType'] == attraction_type]
    high_ratings_attraction = high_ratings_df[high_ratings_df['AttractionType'] == attraction_type]

    # Count the number of ratings for each month
    low_count = low_ratings_attraction['VisitMonth'].value_counts().sort_index()
    high_count = high_ratings_attraction['VisitMonth'].value_counts().sort_index()

    # Merge the counts into a single DataFrame for comparison
    comparison_df = pd.DataFrame({'Low Ratings (1-2)': low_count, 'High Ratings (>2)': high_count}).fillna(0)

    # Plot the comparison
    plt.figure(figsize=(10, 6))
    comparison_df.plot(kind='bar', stacked=False, color=['red', 'green'], edgecolor='black')
    plt.title(f"Monthly Ratings Comparison for Attraction Type: {attraction_type}")
    plt.xlabel('Month')
    plt.ylabel('Number of Ratings')
    plt.xticks(rotation=0)
    plt.grid(axis='y')
    plt.legend(loc='upper right')
    plt.show()

# Step 3: Loop through each attraction type and plot the graphs
unique_attraction_types = df['AttractionType'].unique()
for attraction in unique_attraction_types:
    plot_monthly_ratings_comparison(attraction)

"""*clearly it seems there is no pattern*"""

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/final_mining.csv')

# Remove rows where Rating is 1 or 2
df_cleaned = df[df['Rating'] > 2]

# Print the shape of the dataset before and after removing outliers
print("Original dataset shape:", df.shape)
print("Cleaned dataset shape:", df_cleaned.shape)

# Save the cleaned dataset
df_cleaned.to_csv('/content/cleaned_tourism_data.csv', index=False)
print("Cleaned dataset saved successfully as 'cleaned_tourism_data.csv'")

"""**DATA REDUCTION**"""

import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display, Markdown

# Sample dataset structure (replace this with actual data loading if available)
data = pd.read_csv('/content/cleaned_tourism_data.csv')

# Feature Reduction Analysis as Markdown output
analysis_text = """
# Feature Reduction Analysis

## Features to Keep

*VisitMonth:*
- *Purpose*: Knowing the month of the visit can help capture seasonal patterns and trends. Certain destinations are popular during specific times of the year, and including this information could make recommendations more contextually relevant.
- *Decision*: Keep.

*Rating:*
- *Purpose*: This feature captures the user’s rating for a destination. It provides insights into user preferences, which is fundamental for collaborative filtering (finding similar users) or content-based filtering (suggesting similar attractions).
- *Decision*: Keep.

*Country:*
- *Purpose*: Although country-level data may not fully capture a user’s travel preference, it can help narrow down recommendations to attractions within a specific geographic area, making them more relevant.
- *Decision*: Keep.

*City:*
- *Purpose*: Provides specific information about the destination’s location within the country. This can further refine recommendations to ensure they’re appropriate for users who might prefer certain cities within a country.
- *Decision*: Keep.

*Attraction:*
- *Purpose*: Represents the primary target of the recommendation system—specific tourist attractions. This feature is crucial for tracking and recommending specific places users may be interested in.
- *Decision*: Keep.

*VisitMode:*
- *Purpose*: Visit mode (e.g., solo, family, couples, friends) captures the context of the visit, which can greatly influence recommendations. For instance, a user visiting with family may prefer different attractions than someone visiting solo.
- *Decision*: Keep.

*AttractionType:*
- *Purpose*: This feature describes the type of attraction (e.g., nature, historic). Keeping it allows the recommendation system to suggest attractions of a similar type based on the user's preferences, providing more tailored recommendations.
- *Decision*: Keep.

## Features to Remove

*TransactionId:*
- *Rationale for Removal*: This is a unique identifier for each transaction and is not relevant to understanding user preferences. It doesn’t add value to the recommendation model, as it only tracks individual transactions.
- *Decision*: Remove.

*UserId:*
- *Rationale for Removal*: Since each entry is unique and no user appears multiple times, UserId is redundant. Additionally, we’re focusing on user preferences based on location and attraction type, so UserId is not essential for generating recommendations.
- *Decision*: Remove.

*Continent:*
- *Rationale for Removal*: Each country belongs to a single continent, so this feature does not add additional or unique information when the Country feature is already included. Removing Continent helps reduce redundancy without any significant loss of information.
- *Decision*: Remove.

*Region:*
- *Rationale for Removal*: Similar to Continent, Region is a higher-level geographic identifier that doesn’t provide additional insights beyond what Country already captures. Since each country is associated with a specific region, this feature is also redundant.
- *Decision*: Remove.

*VisitYear:*
- *Rationale for Removal*: While the month of a visit could be relevant for capturing seasonal preferences, the specific year may not be as meaningful in a recommendation system. We’re typically more interested in what types of attractions a user prefers rather than when they visited.
- *Decision*: Remove.

*AttractionAddress:*
- *Rationale for Removal*: The address of an attraction is unique to each destination and doesn’t provide extra information useful for the recommendation system. The Attraction feature already uniquely identifies each place, making AttractionAddress redundant.
- *Decision*: Remove.
"""

# Display the feature reduction analysis as Markdown
display(Markdown(analysis_text))

# Dropping unnecessary features based on the analysis
data_reduced = data.drop(columns=['TransactionId', 'UserId', 'VisitYear', 'Continent', 'Region', 'AttractionAddress'])

# Display the remaining columns after reduction
print("Reduced Dataset Columns:", data_reduced.columns.tolist())

# Visualization: Feature Reduction Summary
initial_features = data.columns.tolist()
reduced_features = data_reduced.columns.tolist()

fig, ax = plt.subplots()
ax.bar(['Before Reduction', 'After Reduction'], [len(initial_features), len(reduced_features)], color=['blue', 'green'])
ax.set_ylabel('Number of Features')
ax.set_title('Feature Reduction Summary')

# Annotate bars with feature count values
for i, v in enumerate([len(initial_features), len(reduced_features)]):
    ax.text(i, v + 0.1, str(v), ha='center', fontweight='bold')

plt.show()

# Save the cleaned dataset
data_reduced.to_csv('/content/reduced_tourism_data.csv', index=False)
print("Reduced dataset saved successfully as 'reduced_tourism_data.csv'")

"""**DATA DISCRETIZATION**

*Convert VisitMonth to Seasons*
"""

df = pd.read_csv('/content/reduced_tourism_data.csv')

# Step 1: Convert VisitMonth to Seasons
def get_season(month):
    if month in [3, 4, 5]:
        return 'Spring'
    elif month in [6, 7, 8]:
        return 'Summer'
    elif month in [9, 10, 11]:
        return 'Fall'
    else:
        return 'Winter'

# Apply the function to create a new column 'Season'
df['Season'] = df['VisitMonth'].apply(get_season)

# Display the first few rows to verify the transformation
print("Transformed DataFrame with Seasons:")
print(df[['VisitMonth', 'Season']].head(10))

"""*  We defined a function called get_season() to map each month to its corresponding season.
*   The function is applied to the VisitMonth column to create a new column called Season.

*   The Season column now represents the seasonal grouping of each visit, which can be more meaningful than simply using the numerical month.

*Convert Rating to Low, Medium, High*
"""

# Step 2: Discretize the Rating column
def categorize_rating(rating):
    if rating <= 2:
        return 'Low'
    elif 3 <= rating <= 4:
        return 'Medium'
    else:
        return 'High'

# Apply the function to create a new column 'RatingCategory'
df['RatingCategory'] = df['Rating'].apply(categorize_rating)

# Display the first few rows to verify the transformation
print("\nTransformed DataFrame with Rating Categories:")
print(df[['Rating', 'RatingCategory']].head(10))

"""*   We defined a function called categorize_rating() to categorize ratings into Low, Medium, and High.
*   The function is applied to the Rating column, creating a new column called RatingCategory

*   The RatingCategory column simplifies the original numerical ratings, making it easier to analyze user satisfaction levels.

*VERIFY AND SAVE*
"""

# Check the distribution of the new Season column
print("\nDistribution of Seasons:")
print(df['Season'].value_counts())

# Check the distribution of the new RatingCategory column
print("\nDistribution of Rating Categories:")
print(df['RatingCategory'].value_counts())

# Save the cleaned and transformed DataFrame to a new CSV file
df.to_csv('/content/cleaned_discretized_tourism_data.csv', index=False)
print("\nCleaned and transformed dataset saved as 'cleaned_discretized_tourism_data.csv'")

"""# **EDA**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, Markdown

# Load the dataset (replace 'path_to_dataset.csv' with your actual file path)
data_path = '/content/reduced_tourism_data.csv'
data = pd.read_csv(data_path, on_bad_lines='skip')

# Check data structure
display(Markdown("### First Few Rows of the Dataset"))
display(data.head())

# Convert 'VisitMonth' to month names if it's numeric (1 for January, 2 for February, etc.)
data['VisitMonth'] = pd.to_datetime(data['VisitMonth'], format='%m').dt.month_name()

### Analysis by Visit Mode ###
# Get unique visit modes from the data
visit_modes = data['VisitMode'].unique()

for mode in visit_modes:
    # Filter data for the specific visit mode
    mode_data = data[data['VisitMode'] == mode]

    # Group by attraction within this mode and count the number of visits
    attraction_counts = mode_data.groupby(['Attraction']).size().reset_index(name='VisitCount')
    attraction_counts = attraction_counts.sort_values(by='VisitCount', ascending=False)  # Sort by visit count

    # Plot for this visit mode
    plt.figure(figsize=(12, 8))
    sns.barplot(x='VisitCount', y='Attraction', data=attraction_counts, palette='viridis')
    plt.title(f"Number of Tourists Visiting Each Attraction for {mode} Travelers")
    plt.xlabel("Number of Tourists")
    plt.ylabel("Attraction")
    plt.show()

    # Display analysis summary for this visit mode
    display(Markdown(f"### Analysis for {mode} Travelers"))
    display(attraction_counts)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, Markdown

# Load the dataset (replace 'path_to_dataset.csv' with your actual file path)
data_path = '/content/reduced_tourism_data.csv'
data = pd.read_csv(data_path, on_bad_lines='skip')

# Check data structure
display(Markdown("### First Few Rows of the Dataset"))
display(data.head())

# Convert 'VisitMonth' to month names if it's numeric (1 for January, 2 for February, etc.)
data['VisitMonth'] = pd.to_datetime(data['VisitMonth'], format='%m').dt.month_name()

### 1. Average Rating per Attraction with Count of Ratings ###
# Calculate average rating and number of ratings for each attraction
rating_summary = data.groupby('Attraction').agg(
    AverageRating=('Rating', 'mean'),
    RatingCount=('Rating', 'size')
).reset_index()

# Sort by average rating for better visualization
rating_summary = rating_summary.sort_values(by='AverageRating', ascending=False)

# Plot average rating with rating count annotated
plt.figure(figsize=(12, 8))
sns.barplot(x='AverageRating', y='Attraction', data=rating_summary, palette='coolwarm')

# Add annotation for the number of ratings
for index, row in rating_summary.iterrows():
    plt.text(row['AverageRating'] + 0.05, index, f"{int(row['RatingCount'])} ratings", va='center')

plt.title("Average Rating and Number of Ratings for Each Attraction")
plt.xlabel("Average Rating")
plt.ylabel("Attraction")
plt.show()

display(Markdown("### Analysis: Average Rating and Number of Ratings per Attraction"))
display(rating_summary)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, HTML

# Load the dataset (replace 'path_to_dataset.csv' with your actual file path)
data_path = '/content/reduced_tourism_data.csv'
data = pd.read_csv(data_path, on_bad_lines='skip')

### Step 1: Filter Countries with Over 1,000 Visits ###
# Explanation:
# We're focusing on countries with more than 1,000 visits because these high-visit countries likely have significant tourist flows to destinations like Bali.
# Analyzing these countries separately allows us to understand specific preferences and trends for countries that are a major source of visitors.
# By identifying top attractions within each of these countries, we can tailor recommendations to meet the needs of these frequent visitors.

# Count the visits per country
country_visit_counts = data['Country'].value_counts()
high_visit_countries = country_visit_counts[country_visit_counts > 1000].index

display(HTML(f"<h2 style='color:#2F4F4F;'>Countries with Over 1,000 Visits</h2><p>{list(high_visit_countries)}</p>"))

### Step 2: Plotting Attraction Visit Counts for Each High-Visit Country ###
# Loop over each high-visit country and create a plot with analysis
for country in high_visit_countries:
    # Filter data for this country
    country_data = data[data['Country'] == country]

    # Count visits per attraction within the country
    attraction_counts = country_data['Attraction'].value_counts().reset_index()
    attraction_counts.columns = ['Attraction', 'VisitCount']

    # Plotting the top attractions for each high-visit country
    plt.figure(figsize=(12, 8))
    sns.barplot(x='VisitCount', y='Attraction', data=attraction_counts, palette='Blues_r')
    plt.title(f"Top Attractions in {country} by Number of Visits", fontsize=16, color="#4B0082")
    plt.xlabel("Number of Visits", fontsize=14)
    plt.ylabel("Attraction", fontsize=14)
    plt.show()

    # Display data table for further analysis
    display(HTML(f"<h3 style='color:#4682B4;'>Visit Counts for Attractions in {country}</h3>"))
    display(attraction_counts)

    # Explanation of the Analysis with HTML for Better Readability in Google Colab
    display(HTML(f"""
    <div style="border:2px solid #4682B4; padding:10px; border-radius:10px; margin-top:20px;">
        <h2 style="color:#2E8B57;">Purpose of Analyzing {country}'s Top Attractions</h2>
        <p>
        Countries with over 1,000 visits represent significant sources of tourists, often comprising a large proportion of visitors to popular destinations like Bali.
        By focusing on these high-visit countries, we gain insights into specific preferences that can enhance the quality of our recommendations for these tourists.
        If a country sends many visitors, it’s helpful to understand their favorite attractions so we can prioritize and highlight those destinations in our recommendations.
        </p>

        <h3 style="color:#2E8B57;">Key Insights and Observations</h3>
        <ul style="list-style-type: circle; color:#333333;">
            <li><b style="color:#4B0082;">Top Attractions for {country}:</b> The most visited attractions by tourists from <b>{country}</b> indicate preferred destinations.
            These can be focal points for personalized recommendations.</li>
            <li><b style="color:#4B0082;">Pattern Recognition:</b> Understanding attraction preferences (e.g., beaches, temples, nature spots)
            provides insight into the type of experiences that resonate with tourists from <b>{country}</b>.</li>
            <li><b style="color:#4B0082;">Opportunities for Targeted Marketing:</b> By identifying the most popular attractions, marketing efforts can
            be directed to emphasize these sites when targeting tourists from <b>{country}</b>.</li>
        </ul>

        <h3 style="color:#2E8B57;">Actionable Takeaways</h3>
        <p>This analysis provides actionable insights to:</p>
        <ul style="list-style-type: square; color:#333333;">
            <li>Enhance **personalized recommendations** by highlighting top attractions for tourists from <b>{country}</b>.</li>
            <li>Support **strategic marketing** by promoting destinations known to be popular among tourists from this country.</li>
            <li>Assist **tourism management** in prioritizing attractions to meet the expectations of visitors from <b>{country}</b>.</li>
        </ul>
    </div>
    """))

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, Markdown

# Load the dataset (replace 'path_to_dataset.csv' with your actual file path)
data_path = '/content/reduced_tourism_data.csv'
data = pd.read_csv(data_path, on_bad_lines='skip')

### 1. Top 20 Countries by Visit Count ###
country_counts = data['Country'].value_counts().nlargest(20).reset_index()
country_counts.columns = ['Country', 'VisitCount']

# Plotting Top 20 Countries by Visit Count
plt.figure(figsize=(12, 8))
sns.barplot(x='VisitCount', y='Country', data=country_counts, palette='viridis')
plt.title("Top 20 Countries by Tourist Visits")
plt.xlabel("Number of Visits")
plt.ylabel("Country")
plt.show()

display(Markdown("### Analysis: Top 20 Countries by Visit Count"))
display(country_counts)

### 2. Top 20 Attractions by Visit Count ###
attraction_counts = data['Attraction'].value_counts().nlargest(20).reset_index()
attraction_counts.columns = ['Attraction', 'VisitCount']

# Plotting Top 20 Attractions by Visit Count
plt.figure(figsize=(12, 8))
sns.barplot(x='VisitCount', y='Attraction', data=attraction_counts, palette='magma')
plt.title("Top 20 Tourist Attractions by Number of Visits")
plt.xlabel("Number of Visits")
plt.ylabel("Attraction")
plt.show()

display(Markdown("### Analysis: Top 20 Attractions by Visit Count"))
display(attraction_counts)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, HTML

# Load the dataset (replace 'path_to_dataset.csv' with your actual file path)
data_path = '/content/cleaned_discretized_tourism_data.csv'
data = pd.read_csv(data_path, on_bad_lines='skip')

### 1. Seasonal Popularity of Attractions ###
# Explanation:
# This analysis explores the popularity of different attractions across the four seasons.
# By understanding seasonal preferences, we can tailor recommendations to suggest attractions that are popular in a particular season.

# Count visits per attraction by season
seasonal_attraction_counts = data.groupby(['Season', 'Attraction']).size().reset_index(name='VisitCount')

# Plotting top attractions for each season
for season in seasonal_attraction_counts['Season'].unique():
    season_data = seasonal_attraction_counts[seasonal_attraction_counts['Season'] == season].nlargest(10, 'VisitCount')

    plt.figure(figsize=(12, 6))
    sns.barplot(x='VisitCount', y='Attraction', data=season_data, palette='coolwarm')
    plt.title(f"Top Attractions in {season}")
    plt.xlabel("Number of Visits")
    plt.ylabel("Attraction")
    plt.show()

    display(HTML(f"""
    <div style="border:2px solid #4682B4; padding:10px; border-radius:10px; margin-top:20px;">
        <h3 style="color:#2E8B57;">Why This Analysis?</h3>
        <p>This plot identifies the most popular attractions during the {season} season, helping to understand which attractions draw the most visitors at different times of the year.</p>

        <h3 style="color:#2E8B57;">Insights</h3>
        <p>By observing the top attractions each season, we can tailor recommendations based on seasonal preferences. For example, if beaches are popular in Summer, we can prioritize beach recommendations during that season.</p>
    </div>
    """))

### 2. Visit Mode Preferences by Season ###
# Explanation:
# We analyze the distribution of visit modes (Solo, Family, Couples, Friends) across each season to understand how tourist preferences vary.
# This information can help in customizing recommendations according to travel context during different times of the year.

season_mode_counts = data.groupby(['Season', 'VisitMode']).size().reset_index(name='VisitCount')

plt.figure(figsize=(12, 8))
sns.barplot(x='Season', y='VisitCount', hue='VisitMode', data=season_mode_counts, palette='viridis')
plt.title("Visit Mode Preferences by Season")
plt.xlabel("Season")
plt.ylabel("Number of Visits")
plt.legend(title="Visit Mode")
plt.show()

display(HTML("""
<div style="border:2px solid #4682B4; padding:10px; border-radius:10px; margin-top:20px;">
    <h3 style="color:#2E8B57;">Why This Analysis?</h3>
    <p>This plot helps us understand the travel context in each season. By knowing which visit modes are popular in each season, we can suggest more relevant attractions. For example, family-friendly locations in Winter or romantic spots in Fall.</p>

    <h3 style="color:#2E8B57;">Insights</h3>
    <p>The distribution of visit modes across seasons reveals if certain travel styles are more popular in specific seasons. For instance, family trips may peak in Summer, while solo travel could be more frequent in Fall.</p>
</div>
"""))

### 3. Average Ratings by Season ###
# Explanation:
# We analyze the average rating of attractions across each season to see if tourist satisfaction changes with the season.

average_rating_by_season = data.groupby('Season')['Rating'].mean().reset_index()

plt.figure(figsize=(10, 6))
sns.barplot(x='Season', y='Rating', data=average_rating_by_season, palette='coolwarm')
plt.title("Average Ratings by Season")
plt.xlabel("Season")
plt.ylabel("Average Rating")
plt.ylim(0, 5)  # assuming a rating scale of 1-5
plt.show()

display(HTML("""
<div style="border:2px solid #4682B4; padding:10px; border-radius:10px; margin-top:20px;">
    <h3 style="color:#2E8B57;">Why This Analysis?</h3>
    <p>This plot highlights how tourists rate their experiences in different seasons. A higher average rating in a particular season could indicate a more enjoyable experience or favorable conditions for tourism.</p>

    <h3 style="color:#2E8B57;">Insights</h3>
    <p>If certain seasons have consistently high ratings, it suggests that tourists generally have better experiences during these times. For example, Spring may yield higher satisfaction due to pleasant weather conditions.</p>
</div>
"""))

### 4. Country-Specific Seasonal Trends ###
# Explanation:
# For each of the top visiting countries, we analyze how tourist visits are distributed across seasons.
# This helps in understanding whether tourists from particular countries prefer certain seasons for travel.

top_countries = country_visit_counts[country_visit_counts > 1000].index  # High-visit countries

for country in top_countries:
    country_season_counts = data[data['Country'] == country]['Season'].value_counts().reset_index()
    country_season_counts.columns = ['Season', 'VisitCount']

    plt.figure(figsize=(8, 5))
    sns.barplot(x='Season', y='VisitCount', data=country_season_counts, palette='magma')
    plt.title(f"Seasonal Visits from {country}")
    plt.xlabel("Season")
    plt.ylabel("Number of Visits")
    plt.show()

    display(HTML(f"""
    <div style="border:2px solid #4682B4; padding:10px; border-radius:10px; margin-top:20px;">
        <h3 style="color:#2E8B57;">Why This Analysis?</h3>
        <p>This analysis explores seasonal visit patterns for tourists from <b>{country}</b>. Understanding when tourists from specific countries prefer to visit can help in planning marketing and promotional efforts targeted at these visitors.</p>

        <h3 style="color:#2E8B57;">Insights</h3>
        <p>If tourists from <b>{country}</b> tend to visit more during certain seasons, tourism strategies can focus on attractions that are popular during those times. For example, if tourists from <b>{country}</b> visit mostly in Summer, beach or outdoor recommendations might be prioritized.</p>
    </div>
    """))

# Count attraction types by season
season_type_counts = data.groupby(['Season', 'AttractionType']).size().reset_index(name='VisitCount')

# Plotting attraction types by season
plt.figure(figsize=(12, 8))
sns.barplot(x='Season', y='VisitCount', hue='AttractionType', data=season_type_counts, palette='Set2')
plt.title("Popularity of Attraction Types by Season")
plt.xlabel("Season")
plt.ylabel("Number of Visits")
plt.legend(title="Attraction Type")
plt.show()

display(HTML("""
<div style="border:2px solid #4682B4; padding:10px; border-radius:10px; margin-top:20px;">
    <h3 style="color:#2E8B57;">Why This Analysis?</h3>
    <p>Identifying which attraction types are popular in each season helps tailor recommendations to match seasonal interests. For example, suggesting beaches during Summer and indoor attractions during Winter.</p>

    <h3 style="color:#2E8B57;">Insights</h3>
    <p>This plot reveals which attraction types resonate with tourists in each season, aiding in more targeted recommendations based on attraction types that align with seasonal preferences.</p>
</div>
"""))

# Count visits by month within each season
monthly_season_counts = data.groupby(['Season', 'VisitMonth']).size().reset_index(name='VisitCount')

plt.figure(figsize=(12, 8))
sns.lineplot(x='VisitMonth', y='VisitCount', hue='Season', data=monthly_season_counts, marker='o', palette='tab10')
plt.title("Monthly Visit Trends within Each Season")
plt.xlabel("Month")
plt.ylabel("Number of Visits")
plt.legend(title="Season")
plt.show()

display(HTML("""
<div style="border:2px solid #4682B4; padding:10px; border-radius:10px; margin-top:20px;">
    <h3 style="color:#2E8B57;">Why This Analysis?</h3>
    <p>This plot allows us to observe monthly trends within each season, identifying peak months for tourist visits.</p>

    <h3 style="color:#2E8B57;">Insights</h3>
    <p>If certain months (e.g., December in Winter) show higher visits, recommendations and promotions can be tailored to encourage visits during these peak months or address crowd management.</p>
</div>
"""))

# Get the top attractions
top_attractions = data['Attraction'].value_counts().head(5).index

# Filter data for only top attractions
top_attraction_data = data[data['Attraction'].isin(top_attractions)]

# Plot monthly visit distribution for each top attraction
plt.figure(figsize=(14, 8))
sns.countplot(x='VisitMonth', hue='Attraction', data=top_attraction_data, palette='tab20')
plt.title("Monthly Visit Distribution for Top Attractions")
plt.xlabel("Month")
plt.ylabel("Number of Visits")
plt.legend(title="Attraction")
plt.show()

display(HTML("""
<div style="border:2px solid #4682B4; padding:10px; border-radius:10px; margin-top:20px;">
    <h3 style="color:#2E8B57;">Insights</h3>
    <ul style="list-style-type: circle;">
        <li><b>Seasonal Peaks:</b> This plot can reveal if specific attractions experience high traffic during certain months. For example, if a beach is most popular in July-August, it’s likely due to summer vacation.</li>
        <li><b>Off-Peak Opportunities:</b> Understanding low-visit months for attractions can help in promoting these destinations during off-peak times.</li>
    </ul>
</div>
"""))

# Filter for high-visit countries and group by country and attraction type
high_visit_country_data = data[data['Country'].isin(top_countries)]
country_type_counts = high_visit_country_data.groupby(['Country', 'AttractionType']).size().reset_index(name='VisitCount')

# Plot attraction type preferences for each high-visit country
plt.figure(figsize=(14, 10))
sns.barplot(x='VisitCount', y='AttractionType', hue='Country', data=country_type_counts, palette='Set3')
plt.title("Attraction Type Preferences by Country")
plt.xlabel("Number of Visits")
plt.ylabel("Attraction Type")
plt.legend(title="Country")
plt.show()

display(HTML("""
<div style="border:2px solid #4682B4; padding:10px; border-radius:10px; margin-top:20px;">
    <h3 style="color:#2E8B57;">Insights</h3>
    <ul style="list-style-type: circle;">
        <li><b>Nature Attractions for Specific Countries:</b> If certain countries, like Canada, show a preference for nature-based attractions, recommendations can emphasize natural parks or scenic locations for those tourists.</li>
        <li><b>Cultural and Historic Preferences:</b> Observing high interest in historical or cultural attractions from countries like Japan may indicate a cultural inclination, guiding targeted recommendations.</li>
    </ul>
</div>
"""))

"""**Correlation Matrix**

*Encode Categorical Variables*
"""

from sklearn.preprocessing import LabelEncoder

# Encode categorical variables using LabelEncoder
label_encoders = {}
for col in ['Country', 'VisitMode', 'City', 'AttractionType', 'Attraction', 'Season', 'RatingCategory']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Display the first few rows to confirm encoding
print("Encoded DataFrame:")
print(df.head())

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the correlation matrix
correlation_matrix = df.corr()

# Set up the matplotlib figure
plt.figure(figsize=(12, 8))

# Draw the heatmap
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

"""### Analysis of Correlation Matrix

The above correlation matrix visualizes the relationships between the various features in our dataset. The correlation values range from -1 to 1, where:

- **1** indicates a perfect positive correlation.
- **-1** indicates a perfect negative correlation.
- **0** indicates no correlation.

#### Key Observations:

1. **Strong Negative Correlation**:
   - The strongest negative correlation (-0.89) is observed between `Rating` and `RatingCategory`. This is expected since `RatingCategory` is derived from the `Rating` column, where a higher numerical rating results in a higher categorical score (Low, Medium, High).


2. **Season vs. VisitMonth**:
   - The correlation between `Season` and `VisitMonth` is moderately negative (-0.38). This is expected because the `Season` column was derived from `VisitMonth`. As a result, these two features are somewhat redundant, and it might be beneficial to keep only one of them.

3. **Low Correlations Across Other Features**:
   - Most other features, such as `Country`, `City`, `VisitMode`, and `AttractionType`, show correlations close to zero with each other, indicating that they are largely independent. This suggests that these features contribute unique information to the dataset and are likely to be valuable for model training.

#### Insights & Recommendations:

- The high correlation between `Rating` and `RatingCategory` indicates redundancy. For model training, it may be beneficial to include only one of these features to avoid multicollinearity.
- The moderate correlation between `Season` and `VisitMonth` suggests that using one of these features alone may suffice, reducing feature redundancy.
- Features with low correlations (close to zero) are likely to provide diverse and independent information, which can improve the predictive power of the model.

*Distribution of Attraction Types*
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Distribution of AttractionType by Country
plt.figure(figsize=(14, 6))
sns.countplot(data=df, x='Country', hue='AttractionType')
plt.title('Attraction Type Distribution by Country')
plt.xticks(rotation=45, ha='right')
plt.show()

# Distribution of AttractionType by VisitMonth
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='VisitMonth', hue='AttractionType')
plt.title('Attraction Type Distribution by Visit Month')
plt.xticks(rotation=45, ha='right')
plt.show()

# Distribution of AttractionType by VisitMode
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='VisitMode', hue='AttractionType')
plt.title('Attraction Type Distribution by Visit Mode')
plt.xticks(rotation=45, ha='right')
plt.show()

"""*Heatmaps to Understand Feature Relationships*"""

df=pd.read_csv('/content/cleaned_discretized_tourism_data.csv')

df.head()

import pandas as pd
import numpy as np

# Convert categorical features to numerical for correlation analysis
from sklearn.preprocessing import LabelEncoder
label_encoders = {}
for col in ['Country', 'VisitMode', 'AttractionType', 'Attraction']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Calculate the correlation matrix
correlation_matrix = df[['Country', 'VisitMonth', 'VisitMode', 'AttractionType', 'Attraction']].corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

"""*Analyzing the Influence of Ratings*"""

plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='AttractionType', y='Rating')
plt.title('Rating Distribution by Attraction Type')
plt.xticks(rotation=45, ha='right')
plt.show()

"""*Analyze Rating vs Inputs for Outlier Detection*"""

plt.figure(figsize=(12, 6))
sns.boxplot(data=df, x='VisitMode', y='Rating', hue='AttractionType')
plt.title('Rating Analysis by Visit Mode and Attraction Type')
plt.xticks(rotation=45, ha='right')
plt.show()

"""**ASSOCIATION RULE MINING**

*Preparing the Data for Association Rule Mining*
"""

import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules

# One-hot encode the categorical features
one_hot_data = pd.get_dummies(df[['Country', 'VisitMonth', 'VisitMode', 'AttractionType', 'Attraction']])

# Display the first few rows to confirm one-hot encoding
print(one_hot_data.head())

# !pip install mlxtend==0.19.0

"""*Applying the Apriori Algorithm*"""

import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules

# Step 1: One-hot encode the categorical columns
categorical_columns = ['Country', 'VisitMonth', 'VisitMode', 'AttractionType', 'Attraction']

# Ensure proper one-hot encoding with binary values
one_hot_data = pd.get_dummies(df[categorical_columns], drop_first=False)

# Step 2: Convert the DataFrame to boolean (True/False) format to satisfy Apriori requirements
one_hot_data = one_hot_data.astype(bool)

# Step 3: Apply the Apriori algorithm
frequent_itemsets = apriori(one_hot_data, min_support=0.01, use_colnames=True)

# Step 4: Generate association rules
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.5)

# Display the generated rules
print("Association Rules:")
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(20))

"""*Filtering and Refining Association Rules*"""

# Filter rules where the consequent is AttractionType (for Task 1)
rules_attraction_type = rules[rules['consequents'].apply(lambda x: 'AttractionType' in str(x))]
print("Rules for predicting AttractionType:")
print(rules_attraction_type[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

# Filter rules where the consequent is Attraction (for Task 2)
rules_attraction = rules[rules['consequents'].apply(lambda x: 'Attraction' in str(x))]
print("\nRules for predicting Attraction:")
print(rules_attraction[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

# Print all the association rules
pd.set_option('display.max_rows', None)  # This will ensure all rows are displayed
print("All Association Rules:")
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

"""# **Association Rule Mining Analysis**

In this section, we utilized **Association Rule Mining** to identify patterns within our dataset and leverage these patterns to enhance the accuracy of our prediction models.

## **Objective of Association Rule Mining**
The goal of applying Association Rule Mining in our project is to:
1. Predict **Attraction Types** based on user inputs: `Country`, `VisitMonth`, and `VisitMode`.
2. Recommend specific **Attractions** given additional inputs: `Country`, `VisitMonth`, `VisitMode`, and `AttractionType`.

## **Key Insights from Association Rules**
After running the Apriori algorithm on our dataset, we generated several association rules that reveal strong relationships between various features. Here are the key findings:

### **1. Country → AttractionType**
- High support (0.845) and confidence (0.845) were observed, indicating that knowing the country can significantly help predict the type of attractions preferred by tourists.
- For example, certain countries are associated with historical sites, while others may be more popular for nature and wildlife activities.

### **2. VisitMonth → AttractionType**
- The month of visit was found to be a strong predictor of attraction types. For instance, tourists visiting during summer months are more likely to explore beaches and outdoor activities.
- This insight can be used to tailor recommendations based on the seasonality of attractions.

### **3. VisitMode → AttractionType**
- The mode of visit (Couples, Friends, Family, Solo) influences the type of attractions tourists prefer. For example, couples might lean towards romantic spots, while families might prefer parks or entertainment attractions.

### **4. Country + AttractionType → Attraction**
- By combining the user's selected `Country` and `AttractionType`, we can effectively narrow down specific attractions with high confidence. This can be particularly useful when recommending top attractions in a country for a given category.

## **How We Will Use These Insights in Our Model**
The insights gained from the association rules will be directly integrated into our recommendation model in the following ways:

### **Model 1**: Predicting Top Attraction Types
- **Inputs**: `Country`, `VisitMonth`, `VisitMode`
- The association rules will help us prioritize the most likely attraction types based on historical data.

### **Model 2**: Predicting Top Attractions
- **Inputs**: `Country`, `VisitMonth`, `VisitMode`, `AttractionType`
- Using the rules, the system will recommend specific attractions within the selected type, taking into account the user's inputs to filter out irrelevant options.

## **Model 1.1**
"""

df=pd.read_csv('/content/cleaned_discretized_tourism_data.csv')

import pandas as pd
import numpy as np

# Convert categorical features to numerical for correlation analysis
from sklearn.preprocessing import LabelEncoder
label_encoders = {}
for col in ['Country', 'VisitMode', 'AttractionType', 'Attraction']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

import pandas as pd

# Assuming your dataset is loaded into the DataFrame 'df'
# Display the first few rows of the dataset
print("Initial Dataset:")
print(df.head())

# Step 1: Create Interaction Features
# Step 1: Convert relevant columns to strings before creating interaction features
df['VisitMode'] = df['VisitMode'].astype(str)
df['Country'] = df['Country'].astype(str)
df['VisitMonth'] = df['VisitMonth'].astype(str)

# Display the updated DataFrame with the new interaction features
print("\nDataset with Interaction Features:")

"""## ***Creating new features "VisitMode_Country", "VisitMode_VisitMonth" from association rule insights ***"""

import pandas as pd

# Assuming the DataFrame 'df' is already loaded and preprocessed

# Step 1: One-Hot Encode 'AttractionType', 'Country', and 'Attraction'
one_hot_attraction_type = pd.get_dummies(df['AttractionType'], prefix='AttractionType').astype(int)
one_hot_country = pd.get_dummies(df['Country'], prefix='Country').astype(int)
one_hot_attraction = pd.get_dummies(df['Attraction'], prefix='Attraction').astype(int)

# Step 2: Add the one-hot encoded columns to the original DataFrame
df_one_hot = pd.concat([df, one_hot_attraction_type, one_hot_country, one_hot_attraction], axis=1)

# Step 3: Create one-hot vectors for each row as lists with integer representation (1/0)
df_one_hot['AttractionType_OneHot'] = one_hot_attraction_type.values.tolist()
df_one_hot['Country_OneHot'] = one_hot_country.values.tolist()
df_one_hot['Attraction_OneHot'] = one_hot_attraction.values.tolist()

# Display the updated DataFrame with one-hot vectors
print("\nDataset with One-Hot Encoded Vectors (using 1/0):")
print(df_one_hot.head())

# Step 4: Display the integer one-hot vectors for verification
print("\nOne-Hot Encoded Vectors for 'AttractionType':")
print(df_one_hot[['AttractionType', 'AttractionType_OneHot']].head())

print("\nOne-Hot Encoded Vectors for 'Country':")
print(df_one_hot[['Country', 'Country_OneHot']].head())

print("\nOne-Hot Encoded Vectors for 'Attraction':")
print(df_one_hot[['Attraction', 'Attraction_OneHot']].head())

import pandas as pd

# Assuming df_one_hot is already preprocessed and contains the following:
# - 'VisitMode' (numerical)
# - 'VisitMonth' (numerical)
# - 'Country_OneHot' (one-hot encoded as a list of integers)

# Step 1: Convert 'VisitMode' and 'VisitMonth' to numerical labels (if not already done)
df_one_hot['VisitMode'] = df_one_hot['VisitMode'].astype(int)
df_one_hot['VisitMonth'] = df_one_hot['VisitMonth'].astype(int)

# Step 2: Create the interaction feature 'VisitMode_VisitMonth' as before
df_one_hot['VisitMode_VisitMonth'] = df_one_hot['VisitMode'] * df_one_hot['VisitMonth']

# Step 3: Generate a single interaction feature 'VisitMode_Country'
# Multiply 'VisitMode' with the one-hot encoded 'Country_OneHot' vector to create a new vector
df_one_hot['VisitMode_Country'] = df_one_hot.apply(
    lambda row: [row['VisitMode'] * val for val in row['Country_OneHot']], axis=1
)

# Display the updated DataFrame with the new interaction feature
print("\nUpdated DataFrame with Interaction Features:")
print(df_one_hot[['VisitMode', 'Country_OneHot', 'VisitMode_Country']].head())

# Display the interaction feature for verification
print("\nExample of 'VisitMode_Country' Interaction Feature:")
print(df_one_hot['VisitMode_Country'].head())

# Step 1: Retain only the relevant columns
columns_to_keep = [
    'AttractionType',
    'Country_OneHot',
    'VisitMode',
    'VisitMonth',
    'VisitMode_VisitMonth',
    'VisitMode_Country'
]

# Filter the DataFrame to only include the columns we want
final_df_cleaned = df_one_hot[columns_to_keep]

# Display the cleaned DataFrame
print("Cleaned Dataset with Only Selected Columns:")
print(final_df_cleaned.head())

# Confirm the shape of the cleaned DataFrame
print("\nShape of the Cleaned DataFrame:", final_df_cleaned.shape)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Step 1: Flatten the one-hot encoded vectors
df_one_hot_expanded = pd.DataFrame(df_one_hot['Country_OneHot'].tolist(), index=df_one_hot.index)
df_one_hot_expanded.columns = [f'Country_{i}' for i in range(df_one_hot_expanded.shape[1])]

df_interaction_expanded = pd.DataFrame(df_one_hot['VisitMode_Country'].tolist(), index=df_one_hot.index)
df_interaction_expanded.columns = [f'VisitMode_Country_{i}' for i in range(df_interaction_expanded.shape[1])]

# Step 2: Merge flattened vectors back into the original DataFrame
df_final = pd.concat([df_one_hot, df_one_hot_expanded, df_interaction_expanded], axis=1)

# Step 3: Prepare features (X) and target (y)

features = ['VisitMode', 'VisitMonth', 'VisitMode_VisitMonth'] + \
           [f'Country_{i}' for i in range(df_one_hot_expanded.shape[1])] + \
           [f'VisitMode_Country_{i}' for i in range(df_interaction_expanded.shape[1])]

X = df_final[features]
y = df_final['AttractionType']

# Step 4: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Train the RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Step 6: Make predictions and evaluate the model
y_pred = model.predict(X_test)

# Step 7: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Assuming y_test and y_pred are already defined
conf_matrix = confusion_matrix(y_test, y_pred)

# Print the confusion matrix
print("\nConfusion Matrix:")
print(conf_matrix)

# Step 9: Plot Confusion Matrix with a darker color map
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, cmap='magma', fmt='g',
            xticklabels=model.classes_, yticklabels=model.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.xticks(rotation=45)
plt.yticks(rotation=45)
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd

def predict_top2_attraction_types(model, visit_mode, visit_month, country, label_encoders, X):
    # Step 1: Encode 'visit_mode' and 'country' using the saved LabelEncoders
    if visit_mode not in label_encoders['VisitMode'].classes_:
        raise ValueError(f"Invalid visit mode: {visit_mode}")
    if country not in label_encoders['Country'].classes_:
        raise ValueError(f"Invalid country: {country}")

    visit_mode_encoded = label_encoders['VisitMode'].transform([visit_mode])[0]
    country_encoded = label_encoders['Country'].transform([country])[0]

    # Step 2: Generate one-hot encoding for 'Country'
    country_one_hot = [0] * len(label_encoders['Country'].classes_)
    country_one_hot[country_encoded] = 1

    # Step 3: Generate interaction feature 'VisitMode_Country'
    visit_mode_country_interaction = [visit_mode_encoded * val for val in country_one_hot]

    # Step 4: Create the interaction feature 'VisitMode_VisitMonth'
    visit_mode_visit_month_interaction = visit_mode_encoded * visit_month

    # Step 5: Assemble the feature vector for prediction
    input_data = pd.DataFrame([[
        visit_mode_encoded,           # VisitMode
        visit_month,                  # VisitMonth
        visit_mode_visit_month_interaction, # VisitMode_VisitMonth
        *country_one_hot,             # Country one-hot encoding
        *visit_mode_country_interaction # VisitMode_Country interaction
    ]], columns=[
        'VisitMode', 'VisitMonth', 'VisitMode_VisitMonth'
    ] + [f'Country_{i}' for i in range(len(country_one_hot))] +
        [f'VisitMode_Country_{i}' for i in range(len(visit_mode_country_interaction))]
    )

    # Step 6: Ensure columns are in the same order as during training
    input_data = input_data.reindex(columns=X.columns, fill_value=0)

    # Step 7: Predict the probabilities for each class
    probabilities = model.predict_proba(input_data)[0]

    # Step 8: Get the indices of the top 2 highest probabilities
    top2_indices = np.argsort(probabilities)[-2:][::-1]  # Get top 2 indices sorted in descending order

    # Step 9: Convert indices to actual labels
    top2_attractions = label_encoders['AttractionType'].inverse_transform(top2_indices)

    # Print the top 2 predictions
    print(f"Top 1 Prediction: {top2_attractions[0]}")
    print(f"Top 2 Prediction: {top2_attractions[1]}")

    return top2_attractions

# Example usage
visit_mode = 'Couples'
visit_month = 7
country = 'France'
top2_predictions = predict_top2_attraction_types(model, visit_mode, visit_month, country, label_encoders, X)

"""****"""

import pandas as pd
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import KFold
from collections import Counter

# Load and preprocess data
file_path = "/content/cleaned_discretized_tourism_data.csv"  # Update with your dataset path
data = pd.read_csv(file_path)

# Encode categorical features
le_country = LabelEncoder()
le_visitmode = LabelEncoder()
le_visitmonth = LabelEncoder()
le_attraction_type = LabelEncoder()  # Encoder for AttractionType

data['Country'] = le_country.fit_transform(data['Country'])
data['VisitMode'] = le_visitmode.fit_transform(data['VisitMode'])
data['VisitMonth'] = le_visitmonth.fit_transform(data['VisitMonth'])
data['AttractionType'] = le_attraction_type.fit_transform(data['AttractionType'])  # Encoding AttractionType

# Prepare input and target variables
X = data[['Country', 'VisitMode', 'VisitMonth']]
y = data['AttractionType']

# Define k-fold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
accuracies = []

# Perform k-fold cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Fit the k-NN model on the training set
    knn = NearestNeighbors(n_neighbors=5, algorithm='auto').fit(X_train)

    # Function to predict the attraction type
    def predict_attraction_type(knn_model, user_features):
        user_features_df = pd.DataFrame([user_features], columns=X.columns)
        distances, indices = knn_model.kneighbors(user_features_df)
        # Get the most common attraction type among the neighbors as the prediction
        recommended_types = y_train.iloc[indices[0]].values
        top_prediction = Counter(recommended_types).most_common(1)[0][0]
        return top_prediction

    # Evaluate accuracy on the test fold
    correct_predictions = 0
    for i in range(len(X_test)):
        user_input = X_test.iloc[i].values
        actual_attraction_type = y_test.iloc[i]
        predicted_attraction_type = predict_attraction_type(knn, user_input)
        if predicted_attraction_type == actual_attraction_type:
            correct_predictions += 1

    # Calculate and store accuracy for this fold
    accuracy = correct_predictions / len(X_test)
    accuracies.append(accuracy)

# Print average accuracy across folds
average_accuracy = sum(accuracies) / len(accuracies)
print(f"Average Accuracy with 5-fold cross-validation: {average_accuracy * 100:.2f}%")

import pandas as pd
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from collections import Counter

# Define the attractions under each type
attraction_mapping = {
    'Cultural & Local Exploration': [
        'Tegalalang Rice Terrace', 'Jodipan Colorful Village', 'Malang City Square',
        'Museum Malang Tempo Doeloe', 'Malioboro Road', 'Water Castle (Tamansari)'
    ],
    'Historical & Cultural': [
        'Uluwatu Temple', 'Tanah Lot Temple', 'Ratu Boko Temple', 'Sewu Temple',
        'Ullen Sentalu Museum', 'Yogyakarta Palace'
    ],
    'Leisure & Entertainment': [
        'Waterbom Bali', 'Bromo Tengger Semeru National Park', 'Khayangan Reflexology & Massage',
        'Kalibiru National Park', 'Ramayana Ballet at Prambanan'
    ],
    'Nature & Wildlife': [
        'Sacred Monkey Forest Sanctuary', 'Seminyak Beach', 'Nusa Dua Beach', 'Sanur Beach',
        'Tegenungan Waterfall', 'Kuta Beach - Bali', 'Mount Semeru Volcano', 'Sempu Island',
        'Balekambang Beach', 'Coban Rondo Waterfall', 'Goa Cina Beach', 'Jomblang Cave', 'Merapi Volcano'
    ]
}

# Load and preprocess data
file_path = "/content/cleaned_discretized_tourism_data.csv"  # Update with your dataset path
data = pd.read_csv(file_path)

# Encode categorical features
le_country = LabelEncoder()
le_visitmode = LabelEncoder()
le_visitmonth = LabelEncoder()
le_attraction_type = LabelEncoder()
le_attraction = LabelEncoder()

data['Country'] = le_country.fit_transform(data['Country'])
data['VisitMode'] = le_visitmode.fit_transform(data['VisitMode'])
data['VisitMonth'] = le_visitmonth.fit_transform(data['VisitMonth'])
data['AttractionType'] = le_attraction_type.fit_transform(data['AttractionType'])
data['Attraction'] = le_attraction.fit_transform(data['Attraction'])

# Split data into training and test sets
X = data[['Country', 'VisitMode', 'VisitMonth', 'AttractionType']]
y = data['Attraction']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the k-NN model on the training set
knn = NearestNeighbors(n_neighbors=5, algorithm='auto').fit(X_train)

# Function to predict the attraction place within a specified type
def predict_attraction(knn_model, user_features, attraction_type):
    user_features_df = pd.DataFrame([user_features], columns=X.columns)
    distances, indices = knn_model.kneighbors(user_features_df)
    # Get attractions within the specified type from nearest neighbors
    recommended_attractions = y_train.iloc[indices[0]].values
    # Filter attractions to match the specified type
    possible_attractions = [
        attraction for attraction in recommended_attractions
        if le_attraction.inverse_transform([attraction])[0] in attraction_mapping[attraction_type]
    ]
    # Select the most common attraction within the type, if available
    if possible_attractions:
        top_prediction = Counter(possible_attractions).most_common(1)[0][0]
    else:
        # Default to the first attraction in the type if no match is found
        top_prediction = le_attraction.transform([attraction_mapping[attraction_type][0]])[0]
    return top_prediction

# Test the function for accuracy on the test set
correct_predictions = 0
for i in range(len(X_test)):
    user_input = X_test.iloc[i].values
    actual_attraction = y_test.iloc[i]
    provided_type = le_attraction_type.inverse_transform([user_input[3]])[0]  # Decode attraction type
    predicted_attraction = predict_attraction(knn, user_input, provided_type)
    if predicted_attraction == actual_attraction:
        correct_predictions += 1

accuracy = correct_predictions / len(X_test)
print(f"Accuracy: {accuracy * 100:.2f}%")

import pandas as pd
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import KFold
from collections import Counter

# Define the attractions under each type
attraction_mapping = {
    'Cultural & Local Exploration': [
        'Tegalalang Rice Terrace', 'Jodipan Colorful Village', 'Malang City Square',
        'Museum Malang Tempo Doeloe', 'Malioboro Road', 'Water Castle (Tamansari)'
    ],
    'Historical & Cultural': [
        'Uluwatu Temple', 'Tanah Lot Temple', 'Ratu Boko Temple', 'Sewu Temple',
        'Ullen Sentalu Museum', 'Yogyakarta Palace'
    ],
    'Leisure & Entertainment': [
        'Waterbom Bali', 'Bromo Tengger Semeru National Park', 'Khayangan Reflexology & Massage',
        'Kalibiru National Park', 'Ramayana Ballet at Prambanan'
    ],
    'Nature & Wildlife': [
        'Sacred Monkey Forest Sanctuary', 'Seminyak Beach', 'Nusa Dua Beach', 'Sanur Beach',
        'Tegenungan Waterfall', 'Kuta Beach - Bali', 'Mount Semeru Volcano', 'Sempu Island',
        'Balekambang Beach', 'Coban Rondo Waterfall', 'Goa Cina Beach', 'Jomblang Cave', 'Merapi Volcano'
    ]
}

# Load and preprocess data
file_path = "/content/cleaned_discretized_tourism_data.csv"  # Update with your dataset path
data = pd.read_csv(file_path)

# Encode categorical features
le_country = LabelEncoder()
le_visitmode = LabelEncoder()
le_visitmonth = LabelEncoder()
le_attraction_type = LabelEncoder()
le_attraction = LabelEncoder()

data['Country'] = le_country.fit_transform(data['Country'])
data['VisitMode'] = le_visitmode.fit_transform(data['VisitMode'])
data['VisitMonth'] = le_visitmonth.fit_transform(data['VisitMonth'])
data['AttractionType'] = le_attraction_type.fit_transform(data['AttractionType'])
data['Attraction'] = le_attraction.fit_transform(data['Attraction'])

# Prepare input and target variables
X = data[['Country', 'VisitMode', 'VisitMonth', 'AttractionType']]
y = data['Attraction']

# Define k-fold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold_accuracies = []
all_predictions = []

# Function to predict the attraction place within a specified type
def predict_attraction(knn_model, user_features, attraction_type):
    user_features_df = pd.DataFrame([user_features], columns=X.columns)
    distances, indices = knn_model.kneighbors(user_features_df)
    # Get attractions within the specified type from nearest neighbors
    recommended_attractions = y_train.iloc[indices[0]].values
    # Filter attractions to match the specified type
    possible_attractions = [
        attraction for attraction in recommended_attractions
        if le_attraction.inverse_transform([attraction])[0] in attraction_mapping[attraction_type]
    ]
    # Select the most common attraction within the type, if available
    if possible_attractions:
        top_prediction = Counter(possible_attractions).most_common(1)[0][0]
    else:
        # Default to the first attraction in the type if no match is found
        top_prediction = le_attraction.transform([attraction_mapping[attraction_type][0]])[0]
    return top_prediction

# Perform k-fold cross-validation
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Fit the k-NN model on the training set
    knn = NearestNeighbors(n_neighbors=5, algorithm='auto').fit(X_train)

    # Evaluate accuracy on the test fold
    correct_predictions = 0
    fold_predictions = []

    for i in range(len(X_test)):
        user_input = X_test.iloc[i].values
        actual_attraction = y_test.iloc[i]
        provided_type = le_attraction_type.inverse_transform([user_input[3]])[0]  # Decode attraction type
        predicted_attraction = predict_attraction(knn, user_input, provided_type)

        # Record the actual vs predicted with the user features
        fold_predictions.append({
            'Country': le_country.inverse_transform([user_input[0]])[0],
            'VisitMode': le_visitmode.inverse_transform([user_input[1]])[0],
            'VisitMonth': le_visitmonth.inverse_transform([user_input[2]])[0],
            'AttractionType': provided_type,
            'Predicted Attraction': le_attraction.inverse_transform([predicted_attraction])[0],
            'Actual Attraction': le_attraction.inverse_transform([actual_attraction])[0]
        })

        if predicted_attraction == actual_attraction:
            correct_predictions += 1

    accuracy = correct_predictions / len(X_test)
    fold_accuracies.append(accuracy)
    all_predictions.extend(fold_predictions)  # Collect predictions for each fold

    print(f"Fold {fold} Accuracy: {accuracy * 100:.2f}%")

# Print average accuracy across folds
average_accuracy = sum(fold_accuracies) / len(fold_accuracies)
print(f"\nAverage Accuracy with 5-fold cross-validation: {average_accuracy * 100:.2f}%\n")

# Display 50 rows of actual vs predicted with original columns
predictions_df = pd.DataFrame(all_predictions).head(50)
print(predictions_df)

import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import NearestNeighbors
from collections import Counter

# Define attractions by type
attraction_mapping = {
    'Cultural & Local Exploration': [
        'Tegalalang Rice Terrace', 'Jodipan Colorful Village', 'Malang City Square',
        'Museum Malang Tempo Doeloe', 'Malioboro Road', 'Water Castle (Tamansari)'
    ],
    'Historical & Cultural': [
        'Uluwatu Temple', 'Tanah Lot Temple', 'Ratu Boko Temple', 'Sewu Temple',
        'Ullen Sentalu Museum', 'Yogyakarta Palace'
    ],
    'Leisure & Entertainment': [
        'Waterbom Bali', 'Bromo Tengger Semeru National Park', 'Khayangan Reflexology & Massage',
        'Kalibiru National Park', 'Ramayana Ballet at Prambanan'
    ],
    'Nature & Wildlife': [
        'Sacred Monkey Forest Sanctuary', 'Seminyak Beach', 'Nusa Dua Beach', 'Sanur Beach',
        'Tegenungan Waterfall', 'Kuta Beach - Bali', 'Mount Semeru Volcano', 'Sempu Island',
        'Balekambang Beach', 'Coban Rondo Waterfall', 'Goa Cina Beach', 'Jomblang Cave', 'Merapi Volcano'
    ]
}

# Load and preprocess data
file_path = "/content/cleaned_discretized_tourism_data.csv"
data = pd.read_csv(file_path)

# Encode categorical features
le_country = LabelEncoder()
le_visitmode = LabelEncoder()
le_visitmonth = LabelEncoder()
le_attraction_type = LabelEncoder()
le_attraction = LabelEncoder()

data['Country'] = le_country.fit_transform(data['Country'])
data['VisitMode'] = le_visitmode.fit_transform(data['VisitMode'])
data['VisitMonth'] = le_visitmonth.fit_transform(data['VisitMonth'])
data['AttractionType'] = le_attraction_type.fit_transform(data['AttractionType'])
data['Attraction'] = le_attraction.fit_transform(data['Attraction'])

# Prepare input and target variables
X = data[['Country', 'VisitMode', 'VisitMonth', 'Rating', 'AttractionType']]
y = data['Attraction']

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the k-NN model on the training set
knn = NearestNeighbors(n_neighbors=5, algorithm='auto').fit(X_train)

# Function to predict an attraction within a specified type based on the features and rating
def predict_attraction(knn_model, user_features, attraction_type):
    user_features_df = pd.DataFrame([user_features], columns=X.columns)
    distances, indices = knn_model.kneighbors(user_features_df)
    # Filter attractions to match the specified type
    recommended_attractions = y_train.iloc[indices[0]].values
    possible_attractions = [
        attraction for attraction in recommended_attractions
        if le_attraction.inverse_transform([attraction])[0] in attraction_mapping[attraction_type]
    ]
    # Select the most common attraction within the type, if available
    if possible_attractions:
        top_prediction = Counter(possible_attractions).most_common(1)[0][0]
    else:
        # Default to the first attraction in the type if no match is found
        top_prediction = le_attraction.transform([attraction_mapping[attraction_type][0]])[0]
    return top_prediction

# Evaluate accuracy on the test set
correct_predictions = 0
for i in range(len(X_test)):
    user_input = X_test.iloc[i].values
    actual_attraction = y_test.iloc[i]
    provided_type = le_attraction_type.inverse_transform([user_input[4]])[0]  # Decode attraction type
    predicted_attraction = predict_attraction(knn, user_input, provided_type)

    if predicted_attraction == actual_attraction:
        correct_predictions += 1

# Calculate accuracy
accuracy = correct_predictions / len(X_test)
print(f"Accuracy: {accuracy * 100:.2f}%")

import pandas as pd
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import NearestNeighbors
from collections import Counter

# Define attractions by type
attraction_mapping = {
    'Cultural & Local Exploration': [
        'Tegalalang Rice Terrace', 'Jodipan Colorful Village', 'Malang City Square',
        'Museum Malang Tempo Doeloe', 'Malioboro Road', 'Water Castle (Tamansari)'
    ],
    'Historical & Cultural': [
        'Uluwatu Temple', 'Tanah Lot Temple', 'Ratu Boko Temple', 'Sewu Temple',
        'Ullen Sentalu Museum', 'Yogyakarta Palace'
    ],
    'Leisure & Entertainment': [
        'Waterbom Bali', 'Bromo Tengger Semeru National Park', 'Khayangan Reflexology & Massage',
        'Kalibiru National Park', 'Ramayana Ballet at Prambanan'
    ],
    'Nature & Wildlife': [
        'Sacred Monkey Forest Sanctuary', 'Seminyak Beach', 'Nusa Dua Beach', 'Sanur Beach',
        'Tegenungan Waterfall', 'Kuta Beach - Bali', 'Mount Semeru Volcano', 'Sempu Island',
        'Balekambang Beach', 'Coban Rondo Waterfall', 'Goa Cina Beach', 'Jomblang Cave', 'Merapi Volcano'
    ]
}

# Load data
file_path = "/content/cleaned_discretized_tourism_data.csv"
data = pd.read_csv(file_path)

# Calculate average rating and count for each attraction
ratings_summary = data.groupby('Attraction').agg(
    avg_rating=('Rating', 'mean'),
    rating_count=('Rating', 'count')
).reset_index()

# Global average rating across all attractions
global_avg_rating = data['Rating'].mean()
lambda_weight = 10  # Adjust based on preference for weighting

# Calculate weighted score for each attraction
ratings_summary['weighted_score'] = (
    (ratings_summary['rating_count'] * ratings_summary['avg_rating'] + lambda_weight * global_avg_rating)
    / (ratings_summary['rating_count'] + lambda_weight)
)

# Merge weighted scores back into the main dataset
data = data.merge(ratings_summary[['Attraction', 'weighted_score']], on='Attraction')

# Shuffle the dataset
data = data.sample(frac=1, random_state=42).reset_index(drop=True)

# Encode categorical features
le_country = LabelEncoder()
le_visitmode = LabelEncoder()
le_visitmonth = LabelEncoder()
le_attraction_type = LabelEncoder()
le_attraction = LabelEncoder()

data['Country'] = le_country.fit_transform(data['Country'])
data['VisitMode'] = le_visitmode.fit_transform(data['VisitMode'])
data['VisitMonth'] = le_visitmonth.fit_transform(data['VisitMonth'])
data['AttractionType'] = le_attraction_type.fit_transform(data['AttractionType'])
data['Attraction'] = le_attraction.fit_transform(data['Attraction'])

# Prepare input and target variables, excluding weighted_score from features
X = data[['Country', 'VisitMode', 'VisitMonth', 'AttractionType']]
y = data['Attraction']

# Define k-fold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold_accuracies = []
all_predictions = []

# Function to predict an attraction within a specified type, applying weighted score as bias
def predict_attraction(knn_model, user_features, attraction_type):
    user_features_df = pd.DataFrame([user_features], columns=X.columns)
    distances, indices = knn_model.kneighbors(user_features_df)

    # Filter attractions to match the specified type
    recommended_attractions = y_train.iloc[indices[0]].values
    possible_attractions = [
        attraction for attraction in recommended_attractions
        if le_attraction.inverse_transform([attraction])[0] in attraction_mapping[attraction_type]
    ]

    # Apply weighted_score as a bias
    if possible_attractions:
        weighted_options = {attr: data[data['Attraction'] == attr]['weighted_score'].iloc[0] for attr in possible_attractions}
        top_prediction = max(weighted_options, key=weighted_options.get)
    else:
        # Default to the highest-weighted attraction in the type if no match is found
        attractions_in_type = data[data['Attraction'].isin(
            le_attraction.transform(attraction_mapping[attraction_type])
        )]
        top_prediction = attractions_in_type.sort_values('weighted_score', ascending=False)['Attraction'].iloc[0]

    return top_prediction

# Perform k-fold cross-validation
for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Fit the k-NN model on the training set
    knn = NearestNeighbors(n_neighbors=5, algorithm='auto').fit(X_train)

    # Evaluate accuracy on the test fold
    correct_predictions = 0
    fold_predictions = []

    for i in range(len(X_test)):
        user_input = X_test.iloc[i].values
        actual_attraction = y_test.iloc[i]
        provided_type = le_attraction_type.inverse_transform([int(user_input[3])])[0]  # Decode attraction type
        predicted_attraction = predict_attraction(knn, user_input, provided_type)

        # Record the actual vs predicted with the user features
        fold_predictions.append({
            'Country': le_country.inverse_transform([int(user_input[0])])[0],
            'VisitMode': le_visitmode.inverse_transform([int(user_input[1])])[0],
            'VisitMonth': le_visitmonth.inverse_transform([int(user_input[2])])[0],
            'AttractionType': provided_type,
            'Predicted Attraction': le_attraction.inverse_transform([predicted_attraction])[0],
            'Actual Attraction': le_attraction.inverse_transform([actual_attraction])[0]
        })

        if predicted_attraction == actual_attraction:
            correct_predictions += 1

    accuracy = correct_predictions / len(X_test)
    fold_accuracies.append(accuracy)
    all_predictions.extend(fold_predictions)  # Collect predictions for each fold

    print(f"Fold {fold} Accuracy: {accuracy * 100:.2f}%")

# Print average accuracy across folds
average_accuracy = sum(fold_accuracies) / len(fold_accuracies)
print(f"\nAverage Accuracy with 5-fold cross-validation: {average_accuracy * 100:.2f}%\n")

# Display 50 rows of actual vs predicted with original columns
predictions_df = pd.DataFrame(all_predictions).head(50)
print(predictions_df)

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from collections import Counter

# Load and shuffle data
file_path = "/content/cleaned_discretized_tourism_data.csv"
data = pd.read_csv(file_path)
data = data.sample(frac=1, random_state=42).reset_index(drop=True)

# Calculate weighted_score based on rating and count
ratings_summary = data.groupby('Attraction').agg(
    avg_rating=('Rating', 'mean'),
    rating_count=('Rating', 'count')
).reset_index()

global_avg_rating = data['Rating'].mean()
lambda_weight = 10

# Weighted score for each attraction
ratings_summary['weighted_score'] = (
    (ratings_summary['rating_count'] * ratings_summary['avg_rating'] + lambda_weight * global_avg_rating)
    / (ratings_summary['rating_count'] + lambda_weight)
)
data = data.merge(ratings_summary[['Attraction', 'weighted_score']], on='Attraction')

# Encode categorical features for similarity calculation
le_country = LabelEncoder()
le_visitmode = LabelEncoder()
le_visitmonth = LabelEncoder()
le_attraction_type = LabelEncoder()
le_attraction = LabelEncoder()

data['Country'] = le_country.fit_transform(data['Country'])
data['VisitMode'] = le_visitmode.fit_transform(data['VisitMode'])
data['VisitMonth'] = le_visitmonth.fit_transform(data['VisitMonth'])
data['AttractionType'] = le_attraction_type.fit_transform(data['AttractionType'])
data['Attraction'] = le_attraction.fit_transform(data['Attraction'])

# Function to safely transform input using label encoders
def safe_transform(encoder, value):
    if value in encoder.classes_:
        return encoder.transform([value])[0]
    else:
        # Use the most frequent value in case of an unseen label
        return encoder.transform([encoder.classes_[0]])[0]

# Function to recommend an attraction based on similarity and weighted score
def recommend_attraction(user_input, attraction_type, alpha=0.1):
    # Filter dataset by provided attraction type
    filtered_data = data[data['AttractionType'] == le_attraction_type.transform([attraction_type])[0]]

    # Calculate exponential probabilities based on weighted_score
    scores = np.array(filtered_data['weighted_score'])
    probabilities = np.exp(alpha * scores)
    probabilities /= probabilities.sum()  # Normalize probabilities to sum to 1

    # Choose attraction based on probabilities
    recommended_attraction = np.random.choice(filtered_data['Attraction'].values, p=probabilities)
    return le_attraction.inverse_transform([recommended_attraction])[0]

# 4-fold cross-validation
kf = KFold(n_splits=4)
fold_accuracies = []

for fold, (train_index, test_index) in enumerate(kf.split(data), 1):
    train, test = data.iloc[train_index], data.iloc[test_index]

    # Initialize correct predictions count for this fold
    correct_predictions = 0

    # Evaluate on each example in the test set
    for i in range(len(test)):
        example_input = {
            'Country': test.iloc[i]['Country'],
            'VisitMode': test.iloc[i]['VisitMode'],
            'VisitMonth': test.iloc[i]['VisitMonth'],
            'AttractionType': test.iloc[i]['AttractionType']
        }

        # Transform user input to encoded values, handling unseen values safely
        user_input = [
            example_input['Country'],
            example_input['VisitMode'],
            example_input['VisitMonth'],
            example_input['AttractionType']
        ]

        # Recommend an attraction and get the actual attraction
        attraction_type_decoded = le_attraction_type.inverse_transform([example_input['AttractionType']])[0]
        predicted_attraction = recommend_attraction(user_input, attraction_type_decoded)
        actual_attraction = le_attraction.inverse_transform([test.iloc[i]['Attraction']])[0]

        # Check if prediction matches actual attraction
        if predicted_attraction == actual_attraction:
            correct_predictions += 1

    # Calculate accuracy for the fold
    fold_accuracy = correct_predictions / len(test)
    fold_accuracies.append(fold_accuracy)
    print(f"Fold {fold} Accuracy: {fold_accuracy * 100:.2f}%")

# Display average accuracy across folds
average_accuracy = np.mean(fold_accuracies)
print(f"\nAverage 4-Fold Accuracy: {average_accuracy * 100:.2f}%")

!pip install scikit-surprise

"""## **Model  3.1**"""

import pandas as pd
from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load and preprocess the dataset
file_path = '/content/cleaned_discretized_tourism_data.csv'  # Update this path to your dataset
data = pd.read_csv(file_path)

# Select and trim relevant columns
data = data[['Country', 'VisitMode', 'AttractionType', 'Attraction']]
data['Country'] = data['Country'].str.strip()
data['VisitMode'] = data['VisitMode'].str.strip()
data['AttractionType'] = data['AttractionType'].str.strip()
data['Attraction'] = data['Attraction'].str.strip()

# Encode categorical features
label_encoders = {}
for column in ['Country', 'VisitMode', 'AttractionType', 'Attraction']:
    le = LabelEncoder()
    data[column] = le.fit_transform(data[column])
    label_encoders[column] = le

# Mapping AttractionType to possible Attraction options
attraction_type_to_attractions = data.groupby('AttractionType')['Attraction'].apply(set).to_dict()

# Separate features and target
X = data[['Country', 'VisitMode', 'AttractionType']]
y = data['Attraction']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Model training with 4-fold cross-validation
rf_model = RandomForestClassifier(random_state=42)
strat_kfold = StratifiedKFold(n_splits=4)

# Cross-validation accuracy scores
cv_scores = cross_val_score(rf_model, X_train, y_train, cv=strat_kfold, scoring='accuracy')
print(f"Stratified Cross-Validation Accuracy Scores: {cv_scores}")
print(f"Mean Stratified Cross-Validation Accuracy: {cv_scores.mean()}")

# Final model training and testing
rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)

# Adjust predictions to ensure they match the user's AttractionType
adjusted_predictions = []
for i in range(len(X_test)):
    predicted_attraction = y_pred[i]
    user_attraction_type = X_test.iloc[i]['AttractionType']

    # Check if predicted attraction is valid for the user's AttractionType
    if predicted_attraction not in attraction_type_to_attractions[user_attraction_type]:
        # If not, select the most probable valid Attraction for the given AttractionType
        valid_attractions = list(attraction_type_to_attractions[user_attraction_type])
        probabilities = rf_model.predict_proba([X_test.iloc[i]])[0]

        # Filter probabilities for only valid attractions
        valid_probabilities = [(attr, prob) for attr, prob in zip(rf_model.classes_, probabilities) if attr in valid_attractions]

        # Select the attraction with the highest probability among valid options
        predicted_attraction = max(valid_probabilities, key=lambda x: x[1])[0]

    adjusted_predictions.append(predicted_attraction)

# Calculate and display adjusted test accuracy
adjusted_test_accuracy = accuracy_score(y_test, adjusted_predictions)
print(f"Adjusted Model Accuracy on Test Data: {adjusted_test_accuracy * 100:.2f}%")

# Display top 50 predictions with actual and adjusted predicted Attraction for inspection
top_50_predictions = pd.DataFrame({
    'Country': label_encoders['Country'].inverse_transform(X_test['Country'][:50]),
    'VisitMode': label_encoders['VisitMode'].inverse_transform(X_test['VisitMode'][:50]),
    'AttractionType': label_encoders['AttractionType'].inverse_transform(X_test['AttractionType'][:50]),
    'Actual Attraction': label_encoders['Attraction'].inverse_transform(y_test[:50]),
    'Adjusted Predicted Attraction': label_encoders['Attraction'].inverse_transform(adjusted_predictions[:50])
})

print("\nTop 50 Adjusted Predictions:\n", top_50_predictions)

import pandas as pd
from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import numpy as np

# Load and preprocess the dataset
file_path = '/content/cleaned_discretized_tourism_data.csv'  # Update this path to your dataset
data = pd.read_csv(file_path)

# Select and trim relevant columns
data = data[['Country', 'VisitMode', 'AttractionType', 'Attraction']]
data['Country'] = data['Country'].str.strip()
data['VisitMode'] = data['VisitMode'].str.strip()
data['AttractionType'] = data['AttractionType'].str.strip()
data['Attraction'] = data['Attraction'].str.strip()

# Encode categorical features
label_encoders = {}
for column in ['Country', 'VisitMode', 'AttractionType', 'Attraction']:
    le = LabelEncoder()
    data[column] = le.fit_transform(data[column])
    label_encoders[column] = le

# Separate features and target
X = data[['Country', 'VisitMode', 'AttractionType']]
y = data['Attraction']

# Dictionary to store separate models for each AttractionType
models = {}
cross_val_scores = {}

# Train a separate model for each AttractionType
for attraction_type in data['AttractionType'].unique():
    # Filter the dataset for the current AttractionType
    type_data = data[data['AttractionType'] == attraction_type]
    X_type = type_data[['Country', 'VisitMode', 'AttractionType']]
    y_type = type_data['Attraction']

    # Train-test split for this specific type
    X_train, X_test, y_train, y_test = train_test_split(X_type, y_type, test_size=0.2, stratify=y_type, random_state=42)

    # Initialize and train RandomForest model for this AttractionType
    rf_model = RandomForestClassifier(random_state=42)
    rf_model.fit(X_train, y_train)

    # Cross-validation for this model
    strat_kfold = StratifiedKFold(n_splits=4)
    cv_scores = cross_val_score(rf_model, X_train, y_train, cv=strat_kfold, scoring='accuracy')
    cross_val_scores[attraction_type] = cv_scores
    attraction_type_name = label_encoders['AttractionType'].inverse_transform([attraction_type])[0]
    print(f"Cross-Validation Scores for AttractionType {attraction_type_name}: {cv_scores}")
    print(f"Mean CV Score: {cv_scores.mean()}")

    # Save the model for this AttractionType
    models[attraction_type] = rf_model

# Function to predict the top 3 attractions based on AttractionType
def predict_top_3_attractions(country, visit_mode, attraction_type):
    # Encode input features
    encoded_country = label_encoders['Country'].transform([country])[0]
    encoded_visit_mode = label_encoders['VisitMode'].transform([visit_mode])[0]
    encoded_attraction_type = label_encoders['AttractionType'].transform([attraction_type])[0]

    # Use the model corresponding to the attraction_type
    model = models[encoded_attraction_type]

    # Get probabilities for all attraction classes
    probabilities = model.predict_proba([[encoded_country, encoded_visit_mode, encoded_attraction_type]])[0]

    # Filter to only the attractions that belong to the specified attraction_type
    attractions_of_type = data[data['AttractionType'] == encoded_attraction_type]['Attraction'].unique()

    # Extract indices and probabilities only for the relevant attractions
    relevant_indices = [i for i in range(len(probabilities)) if i in attractions_of_type]
    relevant_probs = probabilities[relevant_indices]

    # If there are exactly 3 attractions, return all three
    if len(relevant_indices) == 3:
        top_3_attractions = label_encoders['Attraction'].inverse_transform(relevant_indices)
    else:
        # Get top 3 attractions within this type
        top_3_indices = np.argsort(relevant_probs)[-3:][::-1]  # Top 3 highest probabilities
        top_3_attractions = label_encoders['Attraction'].inverse_transform([relevant_indices[i] for i in top_3_indices])

    return top_3_attractions

# Testing with example inputs and displaying top 3 predictions
for attraction_type, model in models.items():
    # Filter test data for the current attraction type
    type_test_data = data[data['AttractionType'] == attraction_type]
    X_type_test = type_test_data[['Country', 'VisitMode', 'AttractionType']]
    y_type_test = type_test_data['Attraction']

    # Show top 3 predictions for the first 5 samples as an example
    for i in range(5):
        sample = X_type_test.iloc[i]
        country = label_encoders['Country'].inverse_transform([sample['Country']])[0]
        visit_mode = label_encoders['VisitMode'].inverse_transform([sample['VisitMode']])[0]
        attraction_type_name = label_encoders['AttractionType'].inverse_transform([sample['AttractionType']])[0]
        top_3_predictions = predict_top_3_attractions(country, visit_mode, attraction_type_name)

        print(f"Sample {i+1} | Country: {country}, VisitMode: {visit_mode}, AttractionType: {attraction_type_name}")
        print(f"Top 3 Predicted Attractions: {top_3_predictions}\n")

# Calculate and display the overall mean test accuracy
test_accuracy_scores = []
for attraction_type, model in models.items():
    # Filter test data for the current attraction type
    type_test_data = data[data['AttractionType'] == attraction_type]
    X_type_test = type_test_data[['Country', 'VisitMode', 'AttractionType']]
    y_type_test = type_test_data['Attraction']

    # Make predictions and calculate accuracy
    y_type_pred = model.predict(X_type_test)
    accuracy = accuracy_score(y_type_test, y_type_pred)
    test_accuracy_scores.append(accuracy)

    # Print accuracy for each AttractionType
    attraction_type_name = label_encoders['AttractionType'].inverse_transform([attraction_type])[0]
    print(f"Test Accuracy for AttractionType {attraction_type_name}: {accuracy * 100:.2f}%")

# Calculate and print the overall mean test accuracy
overall_test_accuracy = sum(test_accuracy_scores) / len(test_accuracy_scores)
print(f"\nOverall Mean Test Accuracy: {overall_test_accuracy * 100:.2f}%")

import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from scipy.stats import uniform

# Load and preprocess the dataset
file_path = '/content/cleaned_discretized_tourism_data.csv'  # Update this path to your dataset
data = pd.read_csv(file_path)

# Select and trim relevant columns
data = data[['Country', 'AttractionType', 'VisitMode', 'Attraction', 'VisitMonth']]
data['Country'] = data['Country'].str.strip()
data['AttractionType'] = data['AttractionType'].str.strip()
data['VisitMode'] = data['VisitMode'].str.strip()
data['Attraction'] = data['Attraction'].str.strip()

# Encode categorical features
label_encoders = {}
for column in ['Country', 'AttractionType', 'VisitMode', 'Attraction']:
    le = LabelEncoder()
    data[column] = le.fit_transform(data[column])
    label_encoders[column] = le

# Adjust VisitMonth to range from 0 to 11
data['VisitMonth'] = data['VisitMonth'] - 1

# Separate features and target, ensuring feature data is in integer type
X = data[['Country', 'AttractionType', 'VisitMode', 'Attraction']].astype(int)
y = data['VisitMonth']  # Target variable: VisitMonth (0-11)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Set up the Logistic Regression model with basic hyperparameters
log_reg_model = LogisticRegression(multi_class='multinomial', solver='saga', random_state=42)

# Define a lightweight parameter grid for regularization
param_distributions = {
    'C': uniform(0.01, 10)  # Regularization strength
}

# Use StratifiedKFold for cross-validation within RandomizedSearchCV
strat_kfold = StratifiedKFold(n_splits=4)
random_search = RandomizedSearchCV(
    estimator=log_reg_model,
    param_distributions=param_distributions,
    cv=strat_kfold,
    n_iter=10,  # Number of random parameter settings to try
    scoring='accuracy',
    verbose=1,
    n_jobs=-1,
    random_state=42
)

# Fit randomized search
random_search.fit(X_train, y_train)

# Print the best parameters and score from random search
print(f"Best Parameters: {random_search.best_params_}")
print(f"Best Cross-Validation Score: {random_search.best_score_}")

# Use the best estimator to make predictions
best_log_reg_model = random_search.best_estimator_
y_pred = best_log_reg_model.predict(X_test)

# Calculate and display test accuracy
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy with Best Logistic Regression Model: {test_accuracy * 100:.2f}%")

# Display classification report for detailed metrics
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=[str(i+1) for i in range(12)]))

# Display top 50 predictions with actual and predicted VisitMonth for inspection
top_50_predictions = pd.DataFrame({
    'Country': label_encoders['Country'].inverse_transform(X_test['Country'][:50]),
    'AttractionType': label_encoders['AttractionType'].inverse_transform(X_test['AttractionType'][:50]),
    'VisitMode': label_encoders['VisitMode'].inverse_transform(X_test['VisitMode'][:50]),
    'Attraction': label_encoders['Attraction'].inverse_transform(X_test['Attraction'][:50]),
    'Actual VisitMonth': (y_test[:50] + 1).values,  # Convert back to 1-12 for display
    'Predicted VisitMonth': (y_pred[:50] + 1)        # Convert back to 1-12 for display
})

print("\nTop 50 Predictions:\n", top_50_predictions)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# Load and preprocess the dataset
file_path = '/content/cleaned_discretized_tourism_data.csv'  # Update this path to your dataset
data = pd.read_csv(file_path)

# Select and trim relevant columns
data = data[['Country', 'AttractionType', 'VisitMode', 'Attraction', 'VisitMonth']]
data['Country'] = data['Country'].str.strip()
data['AttractionType'] = data['AttractionType'].str.strip()
data['VisitMode'] = data['VisitMode'].str.strip()
data['Attraction'] = data['Attraction'].str.strip()

# Encode categorical features
label_encoders = {}
for column in ['Country', 'AttractionType', 'VisitMode', 'Attraction']:
    le = LabelEncoder()
    data[column] = le.fit_transform(data[column])
    label_encoders[column] = le

# Adjust VisitMonth to range from 0 to 11
data['VisitMonth'] = data['VisitMonth'] - 1

# Separate features and target, ensuring feature data is in integer type
X = data[['Country', 'AttractionType', 'VisitMode', 'Attraction']].astype(int)
y = data['VisitMonth']  # Target variable: VisitMonth (0-11)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Initialize and train the Naive Bayes model
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# Predict on the test set
y_pred = nb_model.predict(X_test)

# Calculate and display test accuracy
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy with Naive Bayes Model: {test_accuracy * 100:.2f}%")

# Display classification report for detailed metrics
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=[str(i+1) for i in range(12)]))

# Display top 50 predictions with actual and predicted VisitMonth for inspection
top_50_predictions = pd.DataFrame({
    'Country': label_encoders['Country'].inverse_transform(X_test['Country'][:50]),
    'AttractionType': label_encoders['AttractionType'].inverse_transform(X_test['AttractionType'][:50]),
    'VisitMode': label_encoders['VisitMode'].inverse_transform(X_test['VisitMode'][:50]),
    'Attraction': label_encoders['Attraction'].inverse_transform(X_test['Attraction'][:50]),
    'Actual VisitMonth': (y_test[:50] + 1).values,  # Convert back to 1-12 for display
    'Predicted VisitMonth': (y_pred[:50] + 1)        # Convert back to 1-12 for display
})

print("\nTop 50 Predictions:\n", top_50_predictions)

import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load and preprocess the dataset
file_path = '/content/cleaned_discretized_tourism_data.csv'  # Update this path to your dataset
data = pd.read_csv(file_path)

# Select and trim relevant columns
data = data[['Country', 'VisitMode', 'AttractionType', 'Attraction', 'VisitMonth']]
data['Country'] = data['Country'].str.strip()
data['VisitMode'] = data['VisitMode'].str.strip()
data['AttractionType'] = data['AttractionType'].str.strip()  # Ignored in training
data['Attraction'] = data['Attraction'].str.strip()

# Encode categorical features
label_encoders = {}
for column in ['Country', 'VisitMode', 'Attraction']:
    le = LabelEncoder()
    data[column] = le.fit_transform(data[column])
    label_encoders[column] = le

# Adjust VisitMonth to range from 0 to 11
data['VisitMonth'] = data['VisitMonth'] - 1

# Separate features and target (ignoring AttractionType), ensuring feature data is in integer type
X = data[['Country', 'VisitMode', 'Attraction']].astype(int)
y = data['VisitMonth']  # Target variable: VisitMonth (0-11)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Initialize the Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42)

# Define a simple hyperparameter grid for tuning
param_grid = {
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10]
}

# Use StratifiedKFold for cross-validation within GridSearchCV
strat_kfold = StratifiedKFold(n_splits=4)
grid_search = GridSearchCV(estimator=dt_model, param_grid=param_grid, cv=strat_kfold, scoring='accuracy', verbose=1, n_jobs=-1)

# Fit grid search
grid_search.fit(X_train, y_train)

# Print the best parameters and score from grid search
print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best Cross-Validation Score: {grid_search.best_score_}")

# Use the best estimator to make predictions
best_dt_model = grid_search.best_estimator_
y_pred = best_dt_model.predict(X_test)

# Calculate and display test accuracy
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy with Best Decision Tree Model: {test_accuracy * 100:.2f}%")

# Display classification report for detailed metrics
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=[str(i+1) for i in range(12)]))

# Display top 50 predictions with actual and predicted VisitMonth for inspection
top_50_predictions = pd.DataFrame({
    'Country': label_encoders['Country'].inverse_transform(X_test['Country'][:50]),
    'VisitMode': label_encoders['VisitMode'].inverse_transform(X_test['VisitMode'][:50]),
    'Attraction': label_encoders['Attraction'].inverse_transform(X_test['Attraction'][:50]),
    'Actual VisitMonth': (y_test[:50] + 1).values,  # Convert back to 1-12 for display
    'Predicted VisitMonth': (y_pred[:50] + 1)        # Convert back to 1-12 for display
})

print("\nTop 50 Predictions:\n", top_50_predictions)

import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load and preprocess the dataset
file_path = '/content/cleaned_discretized_tourism_data.csv'  # Update this path to your dataset
data = pd.read_csv(file_path)

# Select and trim relevant columns
data = data[['Country', 'VisitMode', 'Attraction', 'VisitMonth']]
data['Country'] = data['Country'].str.strip()
data['VisitMode'] = data['VisitMode'].str.strip()
data['Attraction'] = data['Attraction'].str.strip()

# Encode categorical features
label_encoders = {}
for column in ['Country', 'VisitMode', 'Attraction']:
    le = LabelEncoder()
    data[column] = le.fit_transform(data[column])
    label_encoders[column] = le

# Adjust VisitMonth to range from 0 to 11
data['VisitMonth'] = data['VisitMonth'] - 1

# Separate features and target, ensuring feature data is in integer type
X = data[['Country', 'VisitMode', 'Attraction']].astype(int)
y = data['VisitMonth']  # Target variable: VisitMonth (0-11)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Initialize the KNN model
knn_model = KNeighborsClassifier()

# Define a parameter grid for tuning the number of neighbors
param_grid = {
    'n_neighbors': [3, 5, 7, 10]
}

# Use StratifiedKFold for cross-validation within GridSearchCV
strat_kfold = StratifiedKFold(n_splits=4)
grid_search = GridSearchCV(estimator=knn_model, param_grid=param_grid, cv=strat_kfold, scoring='accuracy', verbose=1, n_jobs=-1)

# Fit grid search
grid_search.fit(X_train, y_train)

# Print the best parameters and score from grid search
print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best Cross-Validation Score: {grid_search.best_score_}")

# Use the best estimator to make predictions
best_knn_model = grid_search.best_estimator_
y_pred = best_knn_model.predict(X_test)

# Calculate and display test accuracy
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy with Best KNN Model: {test_accuracy * 100:.2f}%")

# Display classification report for detailed metrics
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=[str(i+1) for i in range(12)]))

# Display top 50 predictions with actual and predicted VisitMonth for inspection
top_50_predictions = pd.DataFrame({
    'Country': label_encoders['Country'].inverse_transform(X_test['Country'][:50]),
    'VisitMode': label_encoders['VisitMode'].inverse_transform(X_test['VisitMode'][:50]),
    'Attraction': label_encoders['Attraction'].inverse_transform(X_test['Attraction'][:50]),
    'Actual VisitMonth': (y_test[:50] + 1).values,  # Convert back to 1-12 for display
    'Predicted VisitMonth': (y_pred[:50] + 1)        # Convert back to 1-12 for display
})

print("\nTop 50 Predictions:\n", top_50_predictions)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load and preprocess the dataset
file_path = '/content/cleaned_discretized_tourism_data.csv'  # Update this path to your dataset
data = pd.read_csv(file_path)

# Select and trim relevant columns
data = data[['Country', 'VisitMode', 'Attraction', 'VisitMonth']]
data['Country'] = data['Country'].str.strip()
data['VisitMode'] = data['VisitMode'].str.strip()
data['Attraction'] = data['Attraction'].str.strip()

# Encode categorical features
label_encoders = {}
for column in ['Country', 'VisitMode', 'Attraction']:
    le = LabelEncoder()
    data[column] = le.fit_transform(data[column])
    label_encoders[column] = le

# Adjust VisitMonth to range from 0 to 11
data['VisitMonth'] = data['VisitMonth'] - 1

# Separate features and target
X = data[['Country', 'VisitMode', 'Attraction']].astype(int)
y = data['VisitMonth']  # Target variable: VisitMonth (0-11)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Initialize the LightGBM model with predefined parameters
lgb_model = LGBMClassifier(
    objective='multiclass',
    num_class=12,
    num_leaves=31,
    learning_rate=0.1,
    n_estimators=100,
    max_depth=10,
    random_state=42
)

# Fit the model
lgb_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = lgb_model.predict(X_test)

# Calculate and display test accuracy
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy with LightGBM Model: {test_accuracy * 100:.2f}%")

# Display classification report for detailed metrics
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=[str(i+1) for i in range(12)]))

# Display top 50 predictions with actual and predicted VisitMonth for inspection
top_50_predictions = pd.DataFrame({
    'Country': label_encoders['Country'].inverse_transform(X_test['Country'][:50]),
    'VisitMode': label_encoders['VisitMode'].inverse_transform(X_test['VisitMode'][:50]),
    'Attraction': label_encoders['Attraction'].inverse_transform(X_test['Attraction'][:50]),
    'Actual VisitMonth': (y_test[:50] + 1).values,  # Convert back to 1-12 for display
    'Predicted VisitMonth': (y_pred[:50] + 1)        # Convert back to 1-12 for display
})

print("\nTop 50 Predictions:\n", top_50_predictions)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# Load and preprocess the dataset
file_path = '/content/cleaned_discretized_tourism_data.csv'  # Update this path to your dataset
data = pd.read_csv(file_path)

# Select and trim relevant columns
data = data[['Country', 'VisitMode', 'Attraction', 'VisitMonth']]
data['Country'] = data['Country'].str.strip()
data['VisitMode'] = data['VisitMode'].str.strip()
data['Attraction'] = data['Attraction'].str.strip()

# One-hot encode Country
country_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
country_encoded = country_encoder.fit_transform(data[['Country']])
country_encoded_df = pd.DataFrame(country_encoded, columns=country_encoder.get_feature_names_out(['Country']))

# Encode the other categorical features (VisitMode and Attraction)
label_encoders = {}
for column in ['VisitMode', 'Attraction']:
    le = LabelEncoder()
    data[column] = le.fit_transform(data[column])
    label_encoders[column] = le

# Combine one-hot encoded Country with VisitMode and Attraction
X = pd.concat([country_encoded_df, data[['VisitMode', 'Attraction']]], axis=1)
y = data['VisitMonth'] - 1  # Adjust VisitMonth to range from 0 to 11

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Initialize and fit Random Forest
rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5, random_state=42)
rf_model.fit(X_train, y_train)

# Predict on test set
y_pred = rf_model.predict(X_test)

# Calculate and display test accuracy
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy with Random Forest Model: {test_accuracy * 100:.2f}%")

# Display classification report for detailed metrics
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=[str(i+1) for i in range(12)]))
# Display top 50 predictions with actual and predicted VisitMonth for inspection
top_50_predictions = pd.DataFrame({
    'Country': country_encoder.inverse_transform(X_test[country_encoded_df.columns].iloc[:50]).flatten(),
    'VisitMode': label_encoders['VisitMode'].inverse_transform(X_test['VisitMode'].iloc[:50]),
    'Attraction': label_encoders['Attraction'].inverse_transform(X_test['Attraction'].iloc[:50]),
    'Actual VisitMonth': (y_test.iloc[:50] + 1).values,  # Convert back to 1-12 for display
    'Predicted VisitMonth': (y_pred[:50] + 1)             # Convert back to 1-12 for display
})

print("\nTop 50 Predictions:\n", top_50_predictions)